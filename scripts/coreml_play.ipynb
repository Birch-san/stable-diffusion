{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def nop(it, *a, **k):\n",
    "    if 'prefix' in k:\n",
    "        print(k['prefix'])\n",
    "    return it\n",
    "def noprange(*a, **k):\n",
    "    if 'prefix' in k:\n",
    "        print(k['prefix'])\n",
    "    return range(*a)\n",
    "\n",
    "tqdm.tqdm = nop\n",
    "tqdm.trange = noprange\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, KarrasVeScheduler\n",
    "# from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from PIL import Image\n",
    "\n",
    "kve = KarrasVeScheduler(\n",
    "    sigma_max=14.6146,\n",
    "    # sigma_min=0.0936,\n",
    "    sigma_min=0.0292,\n",
    "    s_churn=0.\n",
    ")\n",
    "# lms = LMSDiscreteScheduler(\n",
    "#   beta_start=0.00085,\n",
    "#   beta_end=0.012,\n",
    "#   beta_schedule=\"scaled_linear\"\n",
    "# )\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"/Users/birch/git/stable-diffusion-v1-4\", safety_checker=None)# torch_type=torch.float16, revision=\"fp16\")\n",
    "# pipe = pipe.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "import diffusers\n",
    "from coremltools.models import MLModel\n",
    "\n",
    "class Undictifier(th.nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "    def forward(self, *args, **kwargs): \n",
    "        return self.m(*args, **kwargs)[\"sample\"]\n",
    "\n",
    "class CLIPUndictifier(th.nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "    def forward(self, *args, **kwargs): \n",
    "        return self.m(*args, **kwargs)[0]\n",
    "\n",
    "def convert_text_encoder(text_encoder, outname):    \n",
    "    import transformers\n",
    "    from transformers.models.clip.modeling_clip import CLIPTextTransformer\n",
    "    CLIPTextTransformer.attention_mask = CLIPTextTransformer._build_causal_attention_mask(None, 1, 77, th.float)\n",
    "    def _fake_build_causal_mask(self, *args, **kwargs):\n",
    "        return self.attention_mask\n",
    "    CLIPTextTransformer._build_causal_attention_mask = _fake_build_causal_mask\n",
    "    f_trace = th.jit.trace(CLIPUndictifier(text_encoder), (th.zeros(1, 77, dtype=th.long)), strict=False, check_trace=False)\n",
    "\n",
    "    f_coreml = ct.convert(f_trace, \n",
    "               inputs=[ct.TensorType(shape=(1, 77))],\n",
    "               convert_to=\"mlprogram\", compute_precision=ct.precision.FLOAT16, skip_model_load=True)\n",
    "    f_coreml.save(outname)\n",
    "\n",
    "def convert_decoder(decoder, outname):    \n",
    "    f_trace = th.jit.trace(decoder, (th.zeros(1, 4, 64, 64)), strict=False, check_trace=False)\n",
    "\n",
    "    f_coreml = ct.convert(f_trace, \n",
    "               inputs=[ct.TensorType(shape=(1, 4, 64, 64))],\n",
    "               convert_to=\"mlprogram\", compute_precision=ct.precision.FLOAT16, skip_model_load=True)\n",
    "    f_coreml.save(outname)\n",
    "\n",
    "def convert_post_quant_conv(layer, outname):\n",
    "    f_trace = th.jit.trace(layer, (th.zeros(1, 4, 64, 64)), strict=False, check_trace=False)\n",
    "\n",
    "    f_coreml = ct.convert(f_trace, \n",
    "            inputs=[ct.TensorType(shape=(1, 4, 64, 64))],\n",
    "            convert_to=\"mlprogram\", compute_precision=ct.precision.FLOAT16, skip_model_load=True)\n",
    "    f_coreml.save(outname)\n",
    "\n",
    "def convert_unet(f, out_name):\n",
    "    from coremltools.converters.mil import Builder as mb\n",
    "    from coremltools.converters.mil.frontend.torch.torch_op_registry import register_torch_op, _TORCH_OPS_REGISTRY\n",
    "    import coremltools.converters.mil.frontend.torch.ops as cml_ops\n",
    "    # def unsliced_attention(self, query, key, value, _sequence_length, _dim):\n",
    "    #     attn = (torch.einsum(\"b i d, b j d -> b i j\", query, key) * self.scale).softmax(dim=-1)\n",
    "    #     attn = torch.einsum(\"b i j, b j d -> b i d\", attn, value)\n",
    "    #     return self.reshape_batch_dim_to_heads(attn)\n",
    "    # diffusers.models.attention.CrossAttention._attention = unsliced_attention\n",
    "    orig_einsum = th.einsum\n",
    "    def fake_einsum(a, b, c):\n",
    "        if a == 'b i d, b j d -> b i j': return th.bmm(b, c.permute(0, 2, 1))\n",
    "        if a == 'b i j, b j d -> b i d': return th.bmm(b, c)\n",
    "        raise ValueError(f\"unsupported einsum {a} on {b.shape} {c.shape}\")\n",
    "    th.einsum = fake_einsum\n",
    "    if \"broadcast_to\" in _TORCH_OPS_REGISTRY: del _TORCH_OPS_REGISTRY[\"broadcast_to\"]\n",
    "    @register_torch_op\n",
    "    def broadcast_to(context, node): return cml_ops.expand(context, node)\n",
    "    if \"gelu\" in _TORCH_OPS_REGISTRY: del _TORCH_OPS_REGISTRY[\"gelu\"]\n",
    "    @register_torch_op\n",
    "    def gelu(context, node): context.add(mb.gelu(x=context[node.inputs[0]], name=node.name))\n",
    "    \n",
    "    print(\"tracing\")\n",
    "    f_trace = th.jit.trace(Undictifier(f), (th.zeros(2, 4, 64, 64), th.zeros(1), th.zeros(2, 77, 768)), strict=False, check_trace=False)\n",
    "    print(\"converting\")\n",
    "    f_coreml_fp16 = ct.convert(f_trace, \n",
    "               inputs=[ct.TensorType(shape=(2, 4, 64, 64)), ct.TensorType(shape=(1,)), ct.TensorType(shape=(2, 77, 768))],\n",
    "               convert_to=\"mlprogram\",  compute_precision=ct.precision.FLOAT16, skip_model_load=True)\n",
    "    f_coreml_fp16.save(f\"{out_name}\")\n",
    "    th.einsum = orig_einsum\n",
    "    \n",
    "class UNetWrapper:\n",
    "    def __init__(self, f, out_name=\"unet.mlpackage\"):\n",
    "        self.in_channels = f.in_channels\n",
    "        if not Path(out_name).exists():\n",
    "            print(\"generating coreml model\"); convert_unet(f, out_name); print(\"saved\")\n",
    "        # not only does ANE take forever to load because it recompiles each time - it then doesn't work!\n",
    "        # and NSLocalizedDescription = \"Error computing NN outputs.\"; is not helpful... GPU it is\n",
    "        print(\"loading saved coreml model\"); f_coreml_fp16 = MLModel(out_name, compute_units=ct.ComputeUnit.CPU_AND_GPU); print(\"loaded\")\n",
    "        self.f = f_coreml_fp16\n",
    "\n",
    "    def __call__(self, sample, timestep, encoder_hidden_states):\n",
    "        from diffusers.models.unet_2d_condition import UNet2DConditionOutput\n",
    "        args = {\"sample\": sample.numpy(), \"timestep\": th.tensor([timestep], dtype=th.int32).numpy(), \"input_35\": encoder_hidden_states.numpy()}\n",
    "        for v in self.f.predict(args).values():\n",
    "            return UNet2DConditionOutput(sample=th.tensor(v, dtype=th.float32))\n",
    "\n",
    "# class TextEncoderWrapper:\n",
    "#     def __init__(self, f, out_name=\"text_encoder.mlpackage\"):\n",
    "#         if not Path(out_name).exists():\n",
    "#             print(\"generating coreml model\"); convert_text_encoder(f, out_name); print(\"saved\")\n",
    "#         print(\"loading saved coreml model\"); self.f = MLModel(out_name, compute_units=ct.ComputeUnit.CPU_AND_GPU); print(\"loaded\")\n",
    "    \n",
    "#     def __call__(self, input):\n",
    "#         args = args = {\"input_ids_1\": input.float().numpy()}\n",
    "#         for v in self.f.predict(args).values():\n",
    "#             return (th.tensor(v, dtype=th.float32),)\n",
    "\n",
    "class DecoderWrapper:\n",
    "    def __init__(self, f, out_name=\"vae_decoder.mlpackage\"):\n",
    "        if not Path(out_name).exists():\n",
    "            print(\"generating coreml model\"); convert_decoder(f, out_name); print(\"saved\")\n",
    "        print(\"loading saved coreml model\"); f_coreml_fp16 = MLModel(out_name, compute_units=ct.ComputeUnit.CPU_AND_GPU); print(\"loaded\")\n",
    "        self.f = f_coreml_fp16\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        args = args = {\"z\": input.numpy()}\n",
    "        for v in self.f.predict(args).values():\n",
    "            return th.tensor(v, dtype=th.float32)\n",
    "\n",
    "class PostQuantConvWrapper:\n",
    "    def __init__(self, f, out_name=\"post_quant_conv.mlpackage\"):\n",
    "        if not Path(out_name).exists():\n",
    "            print(\"generating coreml model\"); convert_post_quant_conv(f, out_name); print(\"saved\")\n",
    "        print(\"loading saved coreml model\"); f_coreml_fp16 = MLModel(out_name, compute_units=ct.ComputeUnit.CPU_AND_GPU); print(\"loaded\")\n",
    "        self.f = f_coreml_fp16\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        args = {\"input\": input.numpy()}\n",
    "        for v in self.f.predict(args).values():\n",
    "            return th.tensor(v, dtype=th.float32)\n",
    "\n",
    "class VAEWrapper:\n",
    "    def __init__(self, decoder, post_quant_conv):\n",
    "        self.decoder = decoder\n",
    "        self.post_quant_conv = post_quant_conv\n",
    "\n",
    "    def decode(self, input):\n",
    "        from diffusers.models.vae import DecoderOutput\n",
    "        quant = self.post_quant_conv(input)\n",
    "        dec = self.decoder(quant)\n",
    "\n",
    "        return DecoderOutput(sample=dec)\n",
    "\n",
    "# pipe.text_encoder = TextEncoderWrapper(pipe.text_encoder)\n",
    "pipe.unet = UNetWrapper(pipe.unet)\n",
    "pipe.vae = VAEWrapper(\n",
    "            DecoderWrapper(pipe.vae.decoder), \n",
    "            PostQuantConvWrapper(pipe.vae.post_quant_conv)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"masterpiece character portrait of a blonde girl, full resolution, 4k, mizuryuu kei, akihiko. yoshida, Pixiv featured, baroque scenic, by artgerm, sylvain sarrailh, rossdraws, wlop, global illumination, vaporwave\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(68673924)\n",
    "image: Image.Image = pipe(\n",
    "\tprompt,\n",
    "\t# guidance_scale=1.,\n",
    "\tgenerator=generator,  \n",
    "  # scheduler=lms,\n",
    "  scheduler=kve,\n",
    "  # num_inference_steps=30\n",
    "  num_inference_steps=15\n",
    ").images[0]\n",
    "\n",
    "sample_path=\"../outputs/diffusers\"\n",
    "base_count = len(os.listdir(sample_path))\n",
    "image.save(os.path.join(sample_path, f\"{base_count:05}.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldmwaifu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a92dc80897b2281210963dc442dd7351ec0107b72c9edf600b7f605562bed7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
