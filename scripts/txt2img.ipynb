{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import fnmatch\n",
    "from functools import partial\n",
    "import cv2\n",
    "from cv2 import randn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor, FloatTensor, uint8\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from PIL.Image import Resampling\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice, repeat as repeat_, chain, pairwise\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast, nn\n",
    "from torch.nn import functional as F\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from random import randint\n",
    "from typing import Generic, Optional, Iterable, List, TypeAlias, Tuple, TypeVar, Callable, Protocol, TypedDict\n",
    "import re\n",
    "from ldm.models.diffusion.ddpm import LatentDiffusion\n",
    "import abc\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "from k_diffusion.sampling import sample_lms, sample_dpm_2, sample_dpm_2_ancestral, sample_euler, sample_euler_ancestral, sample_heun, sample_dpm_fast, sample_dpm_adaptive, get_sigmas_karras, append_zero\n",
    "from k_diffusion.external import CompVisDenoiser\n",
    "\n",
    "def get_device():\n",
    "\tif(torch.cuda.is_available()):\n",
    "\t\treturn 'cuda'\n",
    "\telif(torch.backends.mps.is_available()):\n",
    "\t\treturn 'mps'\n",
    "\telse:\n",
    "\t\treturn 'cpu'\n",
    "\n",
    "\n",
    "class KSamplerCallbackPayload(TypedDict):\n",
    "\tx: FloatTensor\n",
    "\ti: int\n",
    "\tsigma: FloatTensor\n",
    "\tsigma_hat: FloatTensor\n",
    "\tdenoised: FloatTensor\n",
    "\n",
    "KSamplerCallback: TypeAlias = Callable[[KSamplerCallbackPayload], None]\n",
    "\n",
    "T = TypeVar('T')\n",
    "Decorator: TypeAlias = Callable[[T], T]\n",
    "TensorDecorator: TypeAlias = Decorator[Tensor]\n",
    "\n",
    "class DiffusionModel(Protocol):\n",
    "\tscale_factor: float\n",
    "\tdef __call__(self, x: Tensor, sigma: Tensor, **kwargs) -> Tensor: ...\n",
    "\tdef decode_first_stage(self, latents: Tensor) -> Tensor: ...\n",
    "\tdef encode_first_stage(self, pixels: Tensor) -> Tensor: ...\n",
    "\tdef get_first_stage_encoding(self, encoded: Tensor) -> Tensor: ...\n",
    "\n",
    "class DiffusionModelMixin(DiffusionModel):\n",
    "\tscale_factor: float\n",
    "\tinner_model: DiffusionModel\n",
    "\n",
    "\t@property\n",
    "\tdef scale_factor(self) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tnotionally this is the standard deviation of latent values encoded by the autoencoder,\n",
    "\t\tacross the whole dataset upon which it learned.\n",
    "\t\tstable-diffusion just went with the standard deviation of the first batch:\n",
    "\t\t0.18215\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.inner_model.scale_factor\n",
    "\n",
    "\tdef decode_first_stage(self, latents: Tensor) -> Tensor:\n",
    "\t\treturn self.inner_model.decode_first_stage(latents)\n",
    "\n",
    "\tdef encode_first_stage(self, pixels: Tensor) -> Tensor:\n",
    "\t\treturn self.inner_model.encode_first_stage(pixels)\n",
    "\n",
    "\tdef get_first_stage_encoding(self, encoded: Tensor) -> Tensor:\n",
    "\t\treturn self.inner_model.get_first_stage_encoding(encoded)\n",
    "\n",
    "class BaseModelWrapper(nn.Module, DiffusionModelMixin):\n",
    "\tinner_model: DiffusionModel\n",
    "\n",
    "\tdef __init__(self, inner_model: DiffusionModel):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.inner_model = inner_model\n",
    "\t\tDiffusionModelMixin.__init__(self)\n",
    "\n",
    "# workaround until k-diffusion introduces official base model wrapper,\n",
    "# to make the wrapper forward all method calls to the wrapped model\n",
    "# https://github.com/crowsonkb/k-diffusion/pull/23#issuecomment-1239937951\n",
    "\n",
    "class CompVisDenoiserWrapper(CompVisDenoiser, DiffusionModelMixin):\n",
    "\tinner_model: DiffusionModel\n",
    "\n",
    "\tdef __init__(self, model: DiffusionModel, quantize=False):\n",
    "\t\tCompVisDenoiser.__init__(self, model, quantize=quantize)\n",
    "\t\tDiffusionModelMixin.__init__(self)\n",
    "\n",
    "# samplers from the Karras et al paper\n",
    "PRE_KARRAS_K_DIFF_SAMPLERS = {'k_lms', 'dpm2_ancestral', 'euler_ancestral'}\n",
    "KARRAS_SAMPLERS = {'heun', 'euler', 'dpm2'}\n",
    "DPM_SOLVER_SAMPLERS = {'dpm_fast', 'dpm_adaptive'}\n",
    "K_DIFF_SAMPLERS = {*KARRAS_SAMPLERS, *\n",
    "\t\t\t\t   PRE_KARRAS_K_DIFF_SAMPLERS, *DPM_SOLVER_SAMPLERS}\n",
    "NOT_K_DIFF_SAMPLERS = {'ddim', 'plms'}\n",
    "VALID_SAMPLERS = {*K_DIFF_SAMPLERS, *NOT_K_DIFF_SAMPLERS}\n",
    "\n",
    "class KCFGDenoiser(BaseModelWrapper):\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: Tensor,\n",
    "\t\tsigma: Tensor,\n",
    "\t\tuncond: Tensor,\n",
    "\t\tcond: Tensor,\n",
    "\t\tcond_scale: float,\n",
    "\t\tcond_arities: Iterable[int],\n",
    "\t\tcond_weights: Optional[Iterable[float]]\n",
    "\t) -> Tensor:\n",
    "\t\tif uncond is None or cond_scale == 1.0:\n",
    "\t\t\tassert cond is None or cond.size(\n",
    "\t\t\t\tdim=0) == 1, \"multi-cond guidance only implemented when CFG is in-use; please pass in an uncond, or use no more than 1 cond\"\n",
    "\t\t\treturn self.inner_model(x, sigma, cond=cond)\n",
    "\t\tuncond_count = uncond.size(dim=0)\n",
    "\t\tcond_count = cond.size(dim=0)\n",
    "\t\tcond_in = torch.cat((uncond, cond))\n",
    "\t\tdel uncond, cond\n",
    "\t\tcond_arities_tensor = torch.tensor(cond_arities, device=cond_in.device)\n",
    "\t\tx_in = cat_self_with_repeat_interleaved(\n",
    "\t\t\tt=x, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "\t\tdel x\n",
    "\t\tsigma_in = cat_self_with_repeat_interleaved(\n",
    "\t\t\tt=sigma, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "\t\tdel sigma\n",
    "\t\tuncond_out, conds_out = self.inner_model(\n",
    "\t\t\tx_in, sigma_in, cond=cond_in).split([uncond_count, cond_count])\n",
    "\t\tdel x_in, sigma_in, cond_in\n",
    "\t\tunconds = repeat_interleave_along_dim_0(\n",
    "\t\t\tt=uncond_out, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "\t\tdel cond_arities_tensor\n",
    "\t\t# transform\n",
    "\t\t#   tensor([0.5, 0.1])\n",
    "\t\t# into:\n",
    "\t\t#   tensor([[[[0.5000]]],\n",
    "\t\t#           [[[0.1000]]]])\n",
    "\t\tweight_tensor = (torch.tensor(cond_weights, device=uncond_out.device,\n",
    "\t\t\t\t\t\t dtype=unconds.dtype) * cond_scale).reshape(len(cond_weights), 1, 1, 1)\n",
    "\t\tdeltas: Tensor = (conds_out-unconds) * weight_tensor\n",
    "\t\tdel conds_out, unconds, weight_tensor\n",
    "\t\tcond = sum_along_slices_of_dim_0(deltas, arities=cond_arities)\n",
    "\t\tdel deltas\n",
    "\t\treturn uncond_out + cond\n",
    "\n",
    "def chunk(it, size):\n",
    "\tit = iter(it)\n",
    "\treturn iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "@dataclass\n",
    "class InBetweenParams(Generic[T]):\n",
    "\tfrom_: T\n",
    "\tto: T\n",
    "\tstep: float\n",
    "\n",
    "MakeInbetween: TypeAlias = Callable[[InBetweenParams], T]\n",
    "\n",
    "def intersperse_linspace(\n",
    "\tlst: List[T],\n",
    "\tmake_inbetween: MakeInbetween[T],\n",
    "\tsteps: Optional[int]\n",
    ") -> List[T]:\n",
    "\tif steps is None:\n",
    "\t\treturn lst\n",
    "\treturn [\n",
    "\t\t*chain(\n",
    "\t\t\t*(\n",
    "\t\t\t\t(\n",
    "\t\t\t\t\tpair[0],\n",
    "\t\t\t\t\t*(\n",
    "\t\t\t\t\t\tmake_inbetween(\n",
    "\t\t\t\t\t\t\tInBetweenParams(\n",
    "\t\t\t\t\t\t\t\tfrom_=pair[0],\n",
    "\t\t\t\t\t\t\t\tto=pair[1],\n",
    "\t\t\t\t\t\t\t\tstep=step\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t) for step in np.linspace(\n",
    "\t\t\t\t\t\t\tstart=1/steps,\n",
    "\t\t\t\t\t\t\tstop=1,\n",
    "\t\t\t\t\t\t\tnum=steps-1,\n",
    "\t\t\t\t\t\t\tendpoint=False\n",
    "\t\t\t\t\t\t)\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t) for pair in pairwise(lst)\n",
    "\t\t\t)\n",
    "\t\t),\n",
    "\t\tlst[-1]\n",
    "\t]\n",
    "\n",
    "def numpy_to_pil(images):\n",
    "\t\"\"\"\n",
    "\tConvert a numpy image or a batch of images to a PIL image.\n",
    "\t\"\"\"\n",
    "\tif images.ndim == 3:\n",
    "\t\timages = images[None, ...]\n",
    "\timages = (images * 255).round().astype(\"uint8\")\n",
    "\tpil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "\treturn pil_images\n",
    "\n",
    "class LatentsToPils(Protocol):\n",
    "\tdef __call__(x: Tensor) -> Tensor: List[Image.Image]\n",
    "\n",
    "def make_latents_to_pils(model: LatentDiffusion) -> LatentsToPils:\n",
    "\tdef latents_to_pils(latents: Tensor) -> List[Image.Image]:\n",
    "\t\tdecoded: Tensor = model.decode_first_stage(latents)\n",
    "\t\tclamped: Tensor = torch.clamp((decoded + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\t\tdel decoded\n",
    "\t\trearranged: Tensor = rearrange(clamped, 'b c h w -> b h w c')\n",
    "\t\tdel clamped\n",
    "\t\tdenormalized: Tensor = 255. * rearranged\n",
    "\t\tdel rearranged\n",
    "\t\trgb_images = denormalized.cpu().to(dtype=uint8).numpy()\n",
    "\t\tdel denormalized\n",
    "\t\tpils: List[Image.Image] = [Image.fromarray(\n",
    "\t\t\trgb_image) for rgb_image in rgb_images]\n",
    "\t\tdel rgb_images\n",
    "\t\treturn pils\n",
    "\treturn latents_to_pils\n",
    "\n",
    "def repeat_along_dim_0(t: Tensor, factor: int) -> Tensor:\n",
    "\t\"\"\"\n",
    "\tRepeats a tensor's contents along its 0th dim `factor` times.\n",
    "\n",
    "\trepeat_along_dim_0(torch.tensor([[0,1]]), 2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[0, 1]])\n",
    "\t# shape changes from (1, 2)\n",
    "\t#                 to (2, 2)\n",
    "\n",
    "\trepeat_along_dim_0(torch.tensor([[0,1],[2,3]]), 2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3]])\n",
    "\t# shape changes from (2, 2)\n",
    "\t#                 to (4, 2)\n",
    "\t\"\"\"\n",
    "\tassert factor >= 1\n",
    "\tif factor == 1:\n",
    "\t\treturn t\n",
    "\tif t.size(dim=0) == 1:\n",
    "\t\t# prefer expand() whenever we can, since doesn't copy\n",
    "\t\treturn t.expand(factor * t.size(dim=0), *(-1,)*(t.ndim-1))\n",
    "\treturn t.repeat((factor, *(1,)*(t.ndim-1)))\n",
    "\n",
    "def repeat_interleave_along_dim_0(t: Tensor, factors: Iterable[int], factors_tensor: Tensor, output_size: int) -> Tensor:\n",
    "\t\"\"\"\n",
    "\trepeat_interleave()s a tensor's contents along its 0th dim.\n",
    "\n",
    "\tfactors=(2,3)\n",
    "\tfactors_tensor = torch.tensor(factors)\n",
    "\toutput_size=factors_tensor.sum().item() # 5\n",
    "\tt=torch.tensor([[0,1],[2,3]])\n",
    "\trepeat_interleave_along_dim_0(t=t, factors=factors, factors_tensor=factors_tensor, output_size=output_size)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3]])\n",
    "\t\"\"\"\n",
    "\tfactors_len = len(factors)\n",
    "\tassert factors_len >= 1\n",
    "\tif len(factors) == 1:\n",
    "\t\t# prefer repeat() whenever we can, because MPS doesn't support repeat_interleave()\n",
    "\t\treturn repeat_along_dim_0(t, factors[0])\n",
    "\tif t.device.type != 'mps':\n",
    "\t\treturn t.repeat_interleave(factors_tensor, dim=0, output_size=output_size)\n",
    "\treturn torch.cat([repeat_along_dim_0(split, factor) for split, factor in zip(t.split(1, dim=0), factors)])\n",
    "\n",
    "def cat_self_with_repeat_interleaved(t: Tensor, factors: Iterable[int], factors_tensor: Tensor, output_size: int) -> Tensor:\n",
    "\t\"\"\"\n",
    "\tFast-paths for a pattern which in its worst-case looks like:\n",
    "\tt=torch.tensor([[0,1],[2,3]])\n",
    "\tfactors=(2,3)\n",
    "\ttorch.cat((t, t.repeat_interleave(factors, dim=0)))\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3]])\n",
    "\n",
    "\tFast-path:\n",
    "\t  `len(factors) == 1`\n",
    "\t  it's just a normal repeat\n",
    "\tt=torch.tensor([[0,1]])\n",
    "\tfactors=(2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[0, 1]])\n",
    "\n",
    "\tt=torch.tensor([[0,1],[2,3]])\n",
    "\tfactors=(2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3]])\n",
    "\t\"\"\"\n",
    "\tif len(factors) == 1:\n",
    "\t\treturn repeat_along_dim_0(t, factors[0]+1)\n",
    "\treturn torch.cat((t, repeat_interleave_along_dim_0(t=t, factors_tensor=factors_tensor, factors=factors, output_size=output_size)))\n",
    "\n",
    "def sum_along_slices_of_dim_0(t: Tensor, arities: Iterable[int]) -> Tensor:\n",
    "\t\"\"\"\n",
    "\tImplements fast-path for a pattern which in the worst-case looks like this:\n",
    "\tt=torch.tensor([[1],[2],[3]])\n",
    "\tarities=(2,1)\n",
    "\ttorch.cat([torch.sum(split, dim=0, keepdim=True) for split in t.split(arities)])\n",
    "\ttensor([[3],\n",
    "\t\t\t[3]])\n",
    "\n",
    "\tFast-path:\n",
    "\t  `len(arities) == 1`\n",
    "\t  it's just a normal sum(t, dim=0, keepdim=True)\n",
    "\tt=torch.tensor([[1],[2]])\n",
    "\tarities=(2)\n",
    "\tt.sum(dim=0, keepdim=True)\n",
    "\ttensor([[3]])\n",
    "\t\"\"\"\n",
    "\tif len(arities) == 1:\n",
    "\t\tif t.size(dim=0) == 1:\n",
    "\t\t\treturn t\n",
    "\t\treturn t.sum(dim=0, keepdim=True)\n",
    "\tsplits: List[Tensor] = t.split(arities)\n",
    "\tdel t\n",
    "\tsums: List[Tensor] = [\n",
    "\t\ttorch.sum(split, dim=0, keepdim=True) for split in splits]\n",
    "\tdel splits\n",
    "\treturn torch.cat(sums)\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "\tprint(f\"Loading model from {ckpt}\")\n",
    "\tpl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "\tif \"global_step\" in pl_sd:\n",
    "\t\tprint(f\"Global Step: {pl_sd['global_step']}\")\n",
    "\tsd = pl_sd[\"state_dict\"]\n",
    "\tmodel = instantiate_from_config(config.model)\n",
    "\tm, u = model.load_state_dict(sd, strict=False)\n",
    "\tif len(m) > 0 and verbose:\n",
    "\t\tprint(\"missing keys:\")\n",
    "\t\tprint(m)\n",
    "\tif len(u) > 0 and verbose:\n",
    "\t\tprint(\"unexpected keys:\")\n",
    "\t\tprint(u)\n",
    "\n",
    "\tmodel.to(get_device())\n",
    "\tmodel.eval()\n",
    "\treturn model\n",
    "\n",
    "def check_safety_poorly(images, **kwargs):\n",
    "\treturn images, False\n",
    "\n",
    "# https://github.com/lucidrains/imagen-pytorch/blob/ceb23d62ecf611082c82b94f2625d78084738ced/imagen_pytorch/imagen_pytorch.py#L127\n",
    "# from lucidrains' imagen_pytorch\n",
    "# MIT-licensed\n",
    "def right_pad_dims_to(x: Tensor, t: Tensor) -> Tensor:\n",
    "\tpadding_dims = x.ndim - t.ndim\n",
    "\tif padding_dims <= 0:\n",
    "\t\treturn t\n",
    "\treturn t.view(*t.shape, *((1,) * padding_dims))\n",
    "\n",
    "def load_img(path):\n",
    "\timage = Image.open(path).convert(\"RGB\")\n",
    "\tw, h = image.size\n",
    "\tprint(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "\t# resize to integer multiple of 32\n",
    "\tw, h = map(lambda x: x - x % 32, (w, h))\n",
    "\timage = image.resize((w, h), resample=Resampling.LANCZOS)\n",
    "\timage = np.array(image).astype(np.float32) / 255.0\n",
    "\timage = image[None].transpose(0, 3, 1, 2)\n",
    "\timage = torch.from_numpy(image)\n",
    "\treturn 2.*image - 1.\n",
    "\n",
    "# the idea of \"ending the noise ramp early\" (i.e. setting a high sigma_min) is that sigmas as lower as sigma_min\n",
    "# aren't very impactful, and every sigma counts when our step count is low\n",
    "# https://github.com/crowsonkb/k-diffusion/pull/23#issuecomment-1234872495\n",
    "# this is just a more performant way to get the \"sigma before sigma_min\" from a noise schedule, aka\n",
    "# get_sigmas_karras(n=steps, sigma_max=sigma_max, sigma_min=sigma_min_nominal, rho=rho)[-3]\n",
    "def get_premature_sigma_min(\n",
    "\tsteps: int,\n",
    "\tsigma_max: float,\n",
    "\tsigma_min_nominal: float,\n",
    "\trho: float\n",
    ") -> float:\n",
    "\tmin_inv_rho = sigma_min_nominal ** (1 / rho)\n",
    "\tmax_inv_rho = sigma_max ** (1 / rho)\n",
    "\tramp = (steps-2) * 1/(steps-1)\n",
    "\tsigma_min = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n",
    "\treturn sigma_min\n",
    "\n",
    "@dataclass\n",
    "class WeightedPrompt():\n",
    "\ttext: str\n",
    "\tweight: float\n",
    "\n",
    "def parse_prompt(prompt: str) -> WeightedPrompt:\n",
    "\tmatch = re.search(r\"^-?\\d+(\\.\\d*)?:\", prompt)\n",
    "\tif match is None:\n",
    "\t\treturn WeightedPrompt(text=prompt, weight=1.0)\n",
    "\tgroup = match.group()[:-1]\n",
    "\tweight = float(group)\n",
    "\treturn WeightedPrompt(text=prompt[len(group)+1:], weight=weight)\n",
    "\n",
    "MultiPrompt: TypeAlias = Iterable[WeightedPrompt]\n",
    "\n",
    "@dataclass\n",
    "class SampleSpec(abc.ABC):\n",
    "\tmultiprompt: MultiPrompt\n",
    "\n",
    "# https://stackoverflow.com/a/73107990/5257399\n",
    "class InterpStrategy(str, Enum):\n",
    "\tCondDiff = 'cond_diff'\n",
    "\tLatentSlerp = 'slerp'\n",
    "\tLatentLerp = 'lerp'\n",
    "\n",
    "\tdef __str__(self) -> str:\n",
    "\t\treturn self.value\n",
    "\n",
    "@dataclass\n",
    "class BetweenSampleSpec(SampleSpec):\n",
    "\ttarget_multiprompt: MultiPrompt\n",
    "\tinterp_quotient: float\n",
    "\n",
    "SampleSpec.register(BetweenSampleSpec)\n",
    "\n",
    "@dataclass\n",
    "class IdenticalSamplesBatchSpec():\n",
    "\tsample: SampleSpec\n",
    "\n",
    "@dataclass\n",
    "class VariedSamplesBatchSpec():\n",
    "\tsamples: List[SampleSpec]\n",
    "\n",
    "@dataclass\n",
    "class BatchSpec(abc.ABC):\n",
    "\tpass\n",
    "\n",
    "BatchSpec.register(IdenticalSamplesBatchSpec)\n",
    "BatchSpec.register(VariedSamplesBatchSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_threshold(percentile: float, floor: float, t: Tensor) -> Tensor:\n",
    "\tr\"\"\"\n",
    "\tArgs:\n",
    "\t\tpercentile: float between 0.0 and 1.0. for example 0.995 would subject only the top 0.5%ile to clamping.\n",
    "\t\tt: [b, c, h, w] tensor in pixel or latent space.\n",
    "\t\"\"\"\n",
    "\tdevice = t.device\n",
    "\t# https://github.com/lucidrains/imagen-pytorch/blob/ceb23d62ecf611082c82b94f2625d78084738ced/imagen_pytorch/imagen_pytorch.py#L1982\n",
    "\t# adapted from lucidrains' imagen_pytorch (MIT-licensed)\n",
    "\tflattened = rearrange(t, 'b c ... -> b c (...)').abs()\n",
    "\t# aten::sort.values_stable not implemented for MPS\n",
    "\tsort_on_cpu = device.type == 'mps'\n",
    "\tflattened = flattened.cpu() if sort_on_cpu else flattened\n",
    "\t# implementation of pseudocode from Imagen paper https://arxiv.org/abs/2205.11487 Section E, A.32\n",
    "\ts = torch.quantile(\n",
    "\t\tflattened,\n",
    "\t\tpercentile,\n",
    "\t\tdim=2\n",
    "\t)\n",
    "\ts = s.to(device) if sort_on_cpu else s\n",
    "\ts.clamp_(min=floor)\n",
    "\ts = right_pad_dims_to(t, s)\n",
    "\t# MPS complains min and input tensors must be of the same shape\n",
    "\n",
    "\tclamp_tensors_on_cpu = device.type == 'mps'\n",
    "\ts_orig = s\n",
    "\tneg_s = -s\n",
    "\ts = s.cpu() if clamp_tensors_on_cpu else s\n",
    "\tneg_s = neg_s.cpu() if clamp_tensors_on_cpu else neg_s\n",
    "\tpixels = t.cpu() if clamp_tensors_on_cpu else t\n",
    "\tpixels = pixels.clamp(neg_s, s)\n",
    "\tpixels = pixels.to(device) if clamp_tensors_on_cpu else pixels\n",
    "\tpixels = pixels / s_orig\n",
    "\treturn pixels\n",
    "\n",
    "\n",
    "class DynamicThresholdingDenoiser(BaseModelWrapper):\n",
    "\tapply_threshold: TensorDecorator\n",
    "\tfloor: float\n",
    "\n",
    "\tdef __init__(self, model: DiffusionModel, percentile: float, floor: float):\n",
    "\t\tsuper().__init__(model)\n",
    "\t\tself.apply_threshold = partial(dynamic_threshold, percentile, floor)\n",
    "\t\tself.floor = floor\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: FloatTensor,\n",
    "\t\tsigma: FloatTensor,\n",
    "\t\tcond: FloatTensor,\n",
    "\t\t**kwargs,\n",
    "\t) -> FloatTensor:\n",
    "\t\t# thanks to @marunine for explaining how to apply dynamic thresholding to scaled latents\n",
    "\t\tlatents: FloatTensor = self.inner_model(x, sigma, cond=cond, **kwargs)\n",
    "\t\tmagnitude: Tensor = latents.abs().max()\n",
    "\t\tif magnitude < self.floor:\n",
    "\t\t\treturn latents\n",
    "\t\tunscaled: Tensor = latents / self.scale_factor\n",
    "\t\tdel latents\n",
    "\t\tnormalized: Tensor = F.normalize(unscaled, dim=-1)\n",
    "\t\tdel unscaled\n",
    "\t\tthresholded: Tensor = self.apply_threshold(normalized)\n",
    "\t\tdel normalized\n",
    "\t\tdenormalized: Tensor = thresholded * magnitude\n",
    "\t\tdel thresholded, magnitude\n",
    "\t\tnew_latents = denormalized * self.scale_factor\n",
    "\t\tdel denormalized\n",
    "\t\treturn new_latents\n",
    "\n",
    "opt = {\n",
    "\t'prompt': 'masterpiece character portrait of shrine maiden, artgerm, ilya kuvshinov, tony pyykko, from side, looking at viewer, long black hair, upper body, 4k hdr, global illumination, lit from behind, oriental scenic, Pixiv featured, vaporwave',\n",
    "\t'prompt_interpolation_steps': None,\n",
    "\t'prompt_interpolation_strategy': InterpStrategy.CondDiff,\n",
    "\t'outdir': 'outputs/txt2img-samples',\n",
    "\t'skip_grid': True,\n",
    "\t'skip_save': None,\n",
    "\t'steps': 8,\n",
    "\t'sampler': 'heun',\n",
    "\t'karras_noise': True,\n",
    "\t'end_noise_ramp_early': True,\n",
    "\t'sigma_max': None,\n",
    "\t'sigma_min': None,\n",
    "\t'rho': 7.,\n",
    "\t'churn': 0.,\n",
    "\t'dynamic_thresholding': True,\n",
    "\t'dynamic_thresholding_percentile': 0.9995,\n",
    "\t'dynamic_thresholding_floor': 8.8,\n",
    "\t'laion400m': None,\n",
    "\t'fixed_code': None,\n",
    "\t'fixed_code_within_batch': None,\n",
    "\t'ddim_eta': 0.0,\n",
    "\t'n_iter': 1,\n",
    "\t'H': 512,\n",
    "\t'W': 512,\n",
    "\t'C': 4,\n",
    "\t'f': 8,\n",
    "\t'n_samples': 3,\n",
    "\t'n_rows': 0,\n",
    "\t'scale': 7.5,\n",
    "\t'from_file': None,\n",
    "\t'config': 'configs/stable-diffusion/v1-inference.yaml',\n",
    "\t'ckpt': 'models/ldm/stable-diffusion-v1/model.ckpt',\n",
    "\t'seed': 1527468831,\n",
    "\t'precision': 'autocast',\n",
    "\t'filename_prompt': True,\n",
    "\t'filename_sample_ix': None,\n",
    "\t'filename_seed': True,\n",
    "\t'filename_sampling': True,\n",
    "\t'filename_guidance': None,\n",
    "\t'filename_sigmas': None,\n",
    "\t'log_intermediates': True,\n",
    "\t'no_progress_bars': None,\n",
    "\t'init_img': None,\n",
    "\t'strength': 0.75,\n",
    "\t'embedding_path': None,\n",
    "}\n",
    "\n",
    "if opt.laion400m:\n",
    "\tprint(\"Falling back to LAION 400M model...\")\n",
    "\topt.config = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "\topt.ckpt = \"models/ldm/text2img-large/model.ckpt\"\n",
    "\topt.outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model: LatentDiffusion = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "if opt.embedding_path is not None:\n",
    "\tmodel.embedding_manager.load(opt.embedding_path)\n",
    "\n",
    "device = torch.device(get_device())\n",
    "model = model.to(device)\n",
    "\n",
    "latents_to_pils: LatentsToPils = make_latents_to_pils(model)\n",
    "\n",
    "if opt.sampler in K_DIFF_SAMPLERS:\n",
    "\tmodel_k_wrapped = CompVisDenoiserWrapper(model, quantize=True)\n",
    "\tmodel_k_guidance = KCFGDenoiser(model_k_wrapped)\n",
    "elif opt.sampler in NOT_K_DIFF_SAMPLERS:\n",
    "\tif opt.sampler == 'plms':\n",
    "\t\tsampler = PLMSSampler(model)\n",
    "\telse:\n",
    "\t\tsampler = DDIMSampler(model)\n",
    "\n",
    "if opt.dynamic_thresholding:\n",
    "\tmodel_k_guidance = DynamicThresholdingDenoiser(\n",
    "\t\tmodel_k_guidance,\n",
    "\t\topt.dynamic_thresholding_percentile,\n",
    "\t\topt.dynamic_thresholding_floor,\n",
    "\t)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples\n",
    "n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "\n",
    "make_inbetween: MakeInbetween[SampleSpec] = lambda params: BetweenSampleSpec(\n",
    "\tinterp_quotient=params.step,\n",
    "\tmultiprompt=params.from_.multiprompt,\n",
    "\ttarget_multiprompt=params.to.multiprompt\n",
    ")\n",
    "\n",
    "prompts_change_each_batch = bool(opt.from_file) or len(opt.prompt) > 1\n",
    "batch_specs: Iterable[BatchSpec] = None\n",
    "if opt.from_file:\n",
    "\tprint(f\"reading prompts from {opt.from_file}\")\n",
    "\twith open(opt.from_file, \"r\") as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\t\tbatch_specs = [\n",
    "\t\t\tVariedSamplesBatchSpec(samples=batch) for batch in chunk(\n",
    "\t\t\t\tintersperse_linspace(\n",
    "\t\t\t\t\t[\n",
    "\t\t\t\t\t\t# every line in the file is considered to be a single multiprompt.\n",
    "\t\t\t\t\t\tSampleSpec(\n",
    "\t\t\t\t\t\t\t# splitting the line on tab, gives each prompt of the multiprompt.\n",
    "\t\t\t\t\t\t\tmultiprompt=[parse_prompt(\n",
    "\t\t\t\t\t\t\t\tsubprompt) for subprompt in line.split('\\t')]\n",
    "\t\t\t\t\t\t) for line in lines\n",
    "\t\t\t\t\t],\n",
    "\t\t\t\t\tmake_inbetween=make_inbetween,\n",
    "\t\t\t\t\tsteps=opt.prompt_interpolation_steps\n",
    "\t\t\t\t),\n",
    "\t\t\t\tbatch_size\n",
    "\t\t\t)\n",
    "\t\t]\n",
    "elif len(opt.prompt) == 1:\n",
    "\t# fast-path for the common case where just one prompt is provided\n",
    "\tbatch_specs = [IdenticalSamplesBatchSpec(\n",
    "\t\tsample=SampleSpec(\n",
    "\t\t\tmultiprompt=[parse_prompt(subprompt)\n",
    "\t\t\t\t\t\t\tfor subprompt in opt.prompt[0]]\n",
    "\t\t)\n",
    "\t)]\n",
    "else:\n",
    "\tbatch_specs = [\n",
    "\t\tVariedSamplesBatchSpec(samples=batch) for batch in chunk(\n",
    "\t\t\tintersperse_linspace(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\tSampleSpec(\n",
    "\t\t\t\t\t\tmultiprompt=[parse_prompt(subprompt)\n",
    "\t\t\t\t\t\t\t\t\t\tfor subprompt in multiprompt]\n",
    "\t\t\t\t\t) for multiprompt in opt.prompt\n",
    "\t\t\t\t],\n",
    "\t\t\t\tmake_inbetween=make_inbetween,\n",
    "\t\t\t\tsteps=opt.prompt_interpolation_steps\n",
    "\t\t\t),\n",
    "\t\t\tbatch_size\n",
    "\t\t)\n",
    "\t]\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "intermediates_path = os.path.join(outpath, \"intermediates\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "os.makedirs(intermediates_path, exist_ok=True)\n",
    "base_count = len(fnmatch.filter(\n",
    "\tos.listdir(sample_path), f\"{5*'[0-9]'}.*.png\"))\n",
    "grid_count = len(fnmatch.filter(\n",
    "\tos.listdir(outpath), f\"grid-{4*'[0-9]'}.*.png\"))\n",
    "\n",
    "shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "\n",
    "start_code = None\n",
    "\n",
    "karras_noise_active = False\n",
    "end_karras_ramp_early_active = False\n",
    "\n",
    "def _format_sigma_pretty(sigma: Tensor) -> str:\n",
    "\treturn \"%.4f\" % sigma\n",
    "\n",
    "def format_sigmas_pretty(sigmas: Tensor, summary: bool = False) -> str:\n",
    "\tif sigmas is None:\n",
    "\t\treturn '[]'\n",
    "\tif summary and sigmas.size(dim=0) > 9:\n",
    "\t\tstart = \", \".join(_format_sigma_pretty(sigma)\n",
    "\t\t\t\t\t\t\tfor sigma in sigmas[0:4])\n",
    "\t\tend = \", \".join(_format_sigma_pretty(sigma)\n",
    "\t\t\t\t\t\tfor sigma in sigmas[-4:])\n",
    "\t\treturn f'[{start}, â€¦, {end}]'\n",
    "\treturn f'[{\", \".join(_format_sigma_pretty(sigma) for sigma in sigmas)}]'\n",
    "\n",
    "def _compute_common_file_name_portion(seed: int, sigmas: str = '') -> str:\n",
    "\tseed_ = ''\n",
    "\tsampling = ''\n",
    "\tprompt = ''\n",
    "\tsigmas_ = ''\n",
    "\tguidance = ''\n",
    "\tif opt.filename_sampling:\n",
    "\t\tkna = '_kns' if karras_noise_active else ''\n",
    "\t\tnz = '_ek' if end_karras_ramp_early_active else ''\n",
    "\t\tsampling = f\"{opt.sampler}{opt.steps}{kna}{nz}\"\n",
    "\tif opt.filename_seed:\n",
    "\t\tseed_ = f\".s{seed}\"\n",
    "\tif opt.filename_prompt:\n",
    "\t\tsanitized = re.sub(r\"[/\\\\?%*:|\\\"<>\\x7F\\x00-\\x1F]\",\n",
    "\t\t\t\t\t\t\t\"-\", '_'.join(opt.prompt[0]))\n",
    "\t\tprompt = f\"_{sanitized}_\"\n",
    "\tif opt.filename_sigmas and sigmas is not None:\n",
    "\t\tsigmas_ = f\"_{sigmas}_\"\n",
    "\tif opt.filename_guidance:\n",
    "\t\tguidance = f\"_str{opt.strength}_sca{opt.scale}\"\n",
    "\tnominal = f\"{seed_}{prompt}{sigmas_}{guidance}{sampling}\"\n",
    "\t# https://apple.stackexchange.com/a/86617/251820\n",
    "\t# macOS imposes a filename limit of ~255 chars\n",
    "\t# we already used up some on base_count and the file extension\n",
    "\t# shed the biggest parts if we must, so that saving doesn't go bang\n",
    "\tif len(nominal) > 245:\n",
    "\t\tnominal = f\"{seed_}{prompt}{guidance}{sampling}\"\n",
    "\tif len(nominal) > 245:\n",
    "\t\tnominal = f\"{seed_}{guidance}{sampling}\"\n",
    "\treturn nominal\n",
    "\n",
    "def compute_batch_file_name(sigmas: str = '') -> str:\n",
    "\tcommon_file_name_portion = _compute_common_file_name_portion(\n",
    "\t\tseed=opt.seed, sigmas=sigmas)\n",
    "\treturn f\"grid-{grid_count:04}{common_file_name_portion}.png\"\n",
    "\n",
    "def compute_sample_file_name(sample_seed: int, sigmas: Optional[str] = None) -> str:\n",
    "\tcommon_file_name_portion = _compute_common_file_name_portion(\n",
    "\t\tseed=sample_seed, sigmas=sigmas)\n",
    "\treturn f\"{base_count:05}{common_file_name_portion}.png\"\n",
    "\n",
    "def img_to_latent(path: str) -> Tensor:\n",
    "\tassert os.path.isfile(path)\n",
    "\timage = load_img(path).to(device)\n",
    "\timage = repeat(image, '1 ... -> b ...', b=batch_size)\n",
    "\tlatent: Tensor = model.get_first_stage_encoding(\n",
    "\t\tmodel.encode_first_stage(image))  # move to latent space\n",
    "\treturn latent\n",
    "\n",
    "init_latent = None\n",
    "if opt.init_img:\n",
    "\tinit_latent = img_to_latent(opt.init_img)\n",
    "t_enc = int((1.0-opt.strength) * opt.steps)\n",
    "\n",
    "precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
    "if device.type == 'mps':\n",
    "\tprecision_scope = nullcontext  # have to use f32 on mps\n",
    "with torch.no_grad():\n",
    "\twith precision_scope(device.type):\n",
    "\t\twith model.ema_scope():\n",
    "\t\t\ttic = time.perf_counter()\n",
    "\t\t\tall_samples = list()\n",
    "\t\t\tuc: Optional[FloatTensor] = None if opt.scale == 1.0 else model.get_learned_conditioning(\n",
    "\t\t\t\t\"\").expand(batch_size, -1, -1)\n",
    "\t\t\tc: Optional[FloatTensor] = None\n",
    "\t\t\tcond_weights: Optional[Iterable[float]] = None\n",
    "\t\t\tcond_arities: Optional[Iterable[int]] = None\n",
    "\t\t\tsample_seeds: Optional[Iterable[int]] = None\n",
    "\t\t\tfor n in trange(opt.n_iter, desc=\"Iterations\", disable=opt.no_progress_bars):\n",
    "\t\t\t\titer_tic = time.perf_counter()\n",
    "\t\t\t\tfor batch_spec in tqdm(batch_specs, desc=f\"Iteration {n}, batch\", disable=opt.no_progress_bars):\n",
    "\t\t\t\t\tif start_code is None or not opt.fixed_code:\n",
    "\t\t\t\t\t\tfirst_sample_of_batch_seed = opt.seed if opt.seed is not None and (\n",
    "\t\t\t\t\t\t\topt.fixed_code or start_code is None) else randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "\t\t\t\t\t\tif opt.fixed_code_within_batch:\n",
    "\t\t\t\t\t\t\tsample_seeds: Iterable[int] = (\n",
    "\t\t\t\t\t\t\t\tfirst_sample_of_batch_seed,) * opt.n_samples\n",
    "\t\t\t\t\t\t\tseed_everything(first_sample_of_batch_seed)\n",
    "\t\t\t\t\t\t\t# https://github.com/CompVis/stable-diffusion/issues/25#issuecomment-1229706811\n",
    "\t\t\t\t\t\t\t# MPS random is not currently deterministic w.r.t seed, so compute randn() on-CPU\n",
    "\t\t\t\t\t\t\tsample_start_code = torch.randn(shape, device='cpu').to(\n",
    "\t\t\t\t\t\t\t\tdevice) if device.type == 'mps' else torch.randn(shape, device=device)\n",
    "\t\t\t\t\t\t\tstart_code = sample_start_code.unsqueeze(\n",
    "\t\t\t\t\t\t\t\t0).expand(opt.n_samples)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tif opt.n_samples == 1:\n",
    "\t\t\t\t\t\t\t\tsample_seeds: Iterable[int] = (\n",
    "\t\t\t\t\t\t\t\t\tfirst_sample_of_batch_seed,)\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tsample_seeds: Iterable[int] = (first_sample_of_batch_seed,) + tuple(torch.randint(\n",
    "\t\t\t\t\t\t\t\t\tlow=np.iinfo(np.uint32).min,\n",
    "\t\t\t\t\t\t\t\t\thigh=np.iinfo(np.uint32).max,\n",
    "\t\t\t\t\t\t\t\t\tdtype=torch.int64,\n",
    "\t\t\t\t\t\t\t\t\tdevice='cpu',\n",
    "\t\t\t\t\t\t\t\t\tsize=(opt.n_samples-1,)\n",
    "\t\t\t\t\t\t\t\t).numpy())\n",
    "\t\t\t\t\t\t\tsample_start_codes = []\n",
    "\t\t\t\t\t\t\tfor seed in sample_seeds:\n",
    "\t\t\t\t\t\t\t\tseed_everything(seed)\n",
    "\t\t\t\t\t\t\t\t# https://github.com/CompVis/stable-diffusion/issues/25#issuecomment-1229706811\n",
    "\t\t\t\t\t\t\t\t# MPS random is not currently deterministic w.r.t seed, so compute randn() on-CPU\n",
    "\t\t\t\t\t\t\t\tsample_start_codes.append(torch.randn(shape, device='cpu').to(\n",
    "\t\t\t\t\t\t\t\t\tdevice) if device.type == 'mps' else torch.randn(shape, device=device))\n",
    "\t\t\t\t\t\t\tstart_code = torch.stack(\n",
    "\t\t\t\t\t\t\t\tsample_start_codes, dim=0)\n",
    "\n",
    "\t\t\t\t\tif c is None or prompts_change_each_batch:\n",
    "\t\t\t\t\t\tmatch batch_spec:\n",
    "\t\t\t\t\t\t\tcase IdenticalSamplesBatchSpec(sample):\n",
    "\t\t\t\t\t\t\t\t# for some reason Python isn't narrowing the type automatically\n",
    "\t\t\t\t\t\t\t\tsample: SampleSpec = sample\n",
    "\t\t\t\t\t\t\t\ttexts: List[str] = [\n",
    "\t\t\t\t\t\t\t\t\tsubprompt.text for subprompt in sample.multiprompt]\n",
    "\t\t\t\t\t\t\t\tcond_arities: Tuple[int, ...] = (\n",
    "\t\t\t\t\t\t\t\t\tlen(texts),) * batch_size\n",
    "\t\t\t\t\t\t\t\tcond_weights: List[float] = [\n",
    "\t\t\t\t\t\t\t\t\tsubprompt.weight for subprompt in sample.multiprompt] * batch_size\n",
    "\t\t\t\t\t\t\t\tp = model.get_learned_conditioning(texts)\n",
    "\t\t\t\t\t\t\t\tc = repeat_along_dim_0(p, batch_size)\n",
    "\t\t\t\t\t\t\t\tdel p\n",
    "\t\t\t\t\t\t\tcase VariedSamplesBatchSpec(samples):\n",
    "\t\t\t\t\t\t\t\t# for some reason Python isn't narrowing the type automatically\n",
    "\t\t\t\t\t\t\t\tsamples: List[SampleSpec] = samples\n",
    "\t\t\t\t\t\t\t\tassert len(samples) == batch_size\n",
    "\t\t\t\t\t\t\t\ttexts: List[str] = [subprompt.text for sample in samples for subprompt in (\n",
    "\t\t\t\t\t\t\t\t\t(*sample.multiprompt, *sample.target_multiprompt) if isinstance(sample, BetweenSampleSpec) else sample.multiprompt)]\n",
    "\t\t\t\t\t\t\t\tcond_weights: List[float] = [interp_quotient*subprompt.weight for sample in samples for interp_quotient, multiprompt in (((1-sample.interp_quotient, sample.multiprompt), (\n",
    "\t\t\t\t\t\t\t\t\tsample.interp_quotient, sample.target_multiprompt)) if isinstance(sample, BetweenSampleSpec) else ((1., sample.multiprompt),)) for subprompt in multiprompt]\n",
    "\t\t\t\t\t\t\t\tcond_arities: List[int] = [len(sample.multiprompt) + len(sample.target_multiprompt) if isinstance(\n",
    "\t\t\t\t\t\t\t\t\tsample, BetweenSampleSpec) else len(sample.multiprompt) for sample in samples]\n",
    "\t\t\t\t\t\t\t\tc = model.get_learned_conditioning(texts)\n",
    "\t\t\t\t\t\t\tcase _:\n",
    "\t\t\t\t\t\t\t\traise TypeError(\n",
    "\t\t\t\t\t\t\t\t\tf\"That ({batch_spec}) ain't no BatchSpec I ever heard of\")\n",
    "\n",
    "\t\t\t\t\tif opt.sampler in NOT_K_DIFF_SAMPLERS:\n",
    "\t\t\t\t\t\tif opt.karras_noise:\n",
    "\t\t\t\t\t\t\tprint(\n",
    "\t\t\t\t\t\t\t\tf\"[WARN] You have requested --karras_noise, but Karras et al noise schedule is not implemented for {opt.sampler} sampler. Implemented only for {K_DIFF_SAMPLERS}. Using default noise schedule from DDIM.\")\n",
    "\t\t\t\t\t\tif init_latent is None:\n",
    "\t\t\t\t\t\t\tsamples, _ = sampler.sample(\n",
    "\t\t\t\t\t\t\t\tS=opt.steps,\n",
    "\t\t\t\t\t\t\t\tconditioning=c,\n",
    "\t\t\t\t\t\t\t\tbatch_size=opt.n_samples,\n",
    "\t\t\t\t\t\t\t\tshape=shape,\n",
    "\t\t\t\t\t\t\t\tverbose=False,\n",
    "\t\t\t\t\t\t\t\tunconditional_guidance_scale=opt.scale,\n",
    "\t\t\t\t\t\t\t\tunconditional_conditioning=uc,\n",
    "\t\t\t\t\t\t\t\teta=opt.ddim_eta,\n",
    "\t\t\t\t\t\t\t\tx_T=start_code\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t# for PLMS and DDIM, sigmas are all 0\n",
    "\t\t\t\t\t\t\tsigmas = None\n",
    "\t\t\t\t\t\t\tsigmas_quantized = None\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tz_enc = sampler.stochastic_encode(\n",
    "\t\t\t\t\t\t\t\tinit_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "\t\t\t\t\t\t\tsamples = sampler.decode(\n",
    "\t\t\t\t\t\t\t\tz_enc,\n",
    "\t\t\t\t\t\t\t\tc,\n",
    "\t\t\t\t\t\t\t\tt_enc,\n",
    "\t\t\t\t\t\t\t\tunconditional_guidance_scale=opt.scale,\n",
    "\t\t\t\t\t\t\t\tunconditional_conditioning=uc,\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\telif opt.sampler in K_DIFF_SAMPLERS:\n",
    "\t\t\t\t\t\tmatch opt.sampler:\n",
    "\t\t\t\t\t\t\tcase 'dpm_fast':\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_dpm_fast\n",
    "\t\t\t\t\t\t\tcase 'dpm_adaptive':\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_dpm_adaptive\n",
    "\t\t\t\t\t\t\tcase 'dpm2':\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_dpm_2\n",
    "\t\t\t\t\t\t\tcase 'dpm2_ancestral':\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_dpm_2_ancestral\n",
    "\t\t\t\t\t\t\tcase 'heun':\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_heun\n",
    "\t\t\t\t\t\t\tcase 'euler':\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_euler\n",
    "\t\t\t\t\t\t\tcase 'euler_ancestral':\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_euler_ancestral\n",
    "\t\t\t\t\t\t\tcase 'k_lms' | _:\n",
    "\t\t\t\t\t\t\t\tsampling_fn = sample_lms\n",
    "\n",
    "\t\t\t\t\t\tsigmas = None\n",
    "\t\t\t\t\t\tsigmas_quantized = None\n",
    "\t\t\t\t\t\tnoise_schedule_sampler_args = {}\n",
    "\t\t\t\t\t\tif opt.karras_noise or opt.sampler in DPM_SOLVER_SAMPLERS:\n",
    "\t\t\t\t\t\t\t# 0.0292\n",
    "\t\t\t\t\t\t\tsigma_min = opt.sigma_min or model_k_wrapped.sigma_min.item()\n",
    "\t\t\t\t\t\t\t# 14.6146\n",
    "\t\t\t\t\t\t\tsigma_max = opt.sigma_max or model_k_wrapped.sigma_max.item()\n",
    "\t\t\t\t\t\t\tif opt.end_noise_ramp_early and not opt.sigma_min:\n",
    "\t\t\t\t\t\t\t\t# get the \"sigma before sigma_min\" from a slightly longer ramp\n",
    "\t\t\t\t\t\t\t\t# https://github.com/crowsonkb/k-diffusion/pull/23#issuecomment-1234872495\n",
    "\t\t\t\t\t\t\t\tsigma_min = get_premature_sigma_min(\n",
    "\t\t\t\t\t\t\t\t\tsteps=opt.steps+1,\n",
    "\t\t\t\t\t\t\t\t\tsigma_max=sigma_max,\n",
    "\t\t\t\t\t\t\t\t\tsigma_min_nominal=model_k_wrapped.sigma_min.item(),\n",
    "\t\t\t\t\t\t\t\t\trho=opt.rho\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\t\t\t# Karras sampling schedule achieves higher FID in fewer steps\n",
    "\t\t\t\t\t\t\t# https://arxiv.org/abs/2206.00364\n",
    "\t\t\t\t\t\t\tif opt.sampler not in DPM_SOLVER_SAMPLERS:\n",
    "\t\t\t\t\t\t\t\tif opt.sampler not in KARRAS_SAMPLERS:\n",
    "\t\t\t\t\t\t\t\t\tprint(\n",
    "\t\t\t\t\t\t\t\t\t\tf\"[WARN] you have enabled --karras_noise, but you are using it with a sampler ({opt.sampler}) outside of the ones proposed in the same paper (arXiv:2206.00364), {KARRAS_SAMPLERS}. No idea what results you'll get.\")\n",
    "\n",
    "\t\t\t\t\t\t\t\tsigmas = get_sigmas_karras(\n",
    "\t\t\t\t\t\t\t\t\tn=opt.steps,\n",
    "\t\t\t\t\t\t\t\t\tsigma_min=sigma_min,\n",
    "\t\t\t\t\t\t\t\t\tsigma_max=sigma_max,\n",
    "\t\t\t\t\t\t\t\t\trho=opt.rho,\n",
    "\t\t\t\t\t\t\t\t\tdevice=device,\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\tkarras_noise_active = True\n",
    "\t\t\t\t\t\t\t\tend_karras_ramp_early_active = opt.end_noise_ramp_early\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tif opt.sampler in KARRAS_SAMPLERS:\n",
    "\t\t\t\t\t\t\t\tprint(\n",
    "\t\t\t\t\t\t\t\t\tf\"[WARN] you should really enable --karras_noise for best results; it's the noise schedule proposed in the same paper (arXiv:2206.00364) as the sampler you're using ({opt.sampler}). Falling back to default k-diffusion get_sigmas() noise schedule.\")\n",
    "\t\t\t\t\t\t\tsigmas = model_k_wrapped.get_sigmas(opt.steps)\n",
    "\n",
    "\t\t\t\t\t\tif opt.sampler in KARRAS_SAMPLERS:\n",
    "\t\t\t\t\t\t\tnoise_schedule_sampler_args['s_churn'] = opt.churn\n",
    "\n",
    "\t\t\t\t\t\tif init_latent is not None:\n",
    "\t\t\t\t\t\t\tassert opt.sampler not in DPM_SOLVER_SAMPLERS, \"img2img not yet implemented for DPM-Solver samplers -- need to figure out how to skip a portion of the noise schedule\"\n",
    "\t\t\t\t\t\t\tsigmas = sigmas[len(sigmas) - t_enc - 1:]\n",
    "\n",
    "\t\t\t\t\t\tif opt.sampler in DPM_SOLVER_SAMPLERS:\n",
    "\t\t\t\t\t\t\tnoise_schedule_sampler_args['sigma_min'] = sigma_min\n",
    "\t\t\t\t\t\t\tnoise_schedule_sampler_args['sigma_max'] = sigma_max\n",
    "\t\t\t\t\t\t\tif opt.sampler == 'dpm_fast':\n",
    "\t\t\t\t\t\t\t\tnoise_schedule_sampler_args['n'] = opt.steps\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tnoise_schedule_sampler_args['sigmas'] = sigmas\n",
    "\t\t\t\t\t\t\tprint('sigmas (before quantization):')\n",
    "\t\t\t\t\t\t\tprint(format_sigmas_pretty(sigmas))\n",
    "\t\t\t\t\t\t\tprint('sigmas (after quantization):')\n",
    "\t\t\t\t\t\t\tsigmas_quantized = append_zero(model_k_wrapped.sigmas[torch.argmin(\n",
    "\t\t\t\t\t\t\t\t(sigmas[:-1].reshape(len(sigmas)-1, 1).repeat(1, len(model_k_wrapped.sigmas)) - model_k_wrapped.sigmas).abs(), dim=1)])\n",
    "\t\t\t\t\t\t\tprint(format_sigmas_pretty(sigmas_quantized))\n",
    "\n",
    "\t\t\t\t\t\tfirst_sigma = sigma_max if opt.sampler in DPM_SOLVER_SAMPLERS else sigmas[\n",
    "\t\t\t\t\t\t\t0]\n",
    "\t\t\t\t\t\tx = start_code * first_sigma\n",
    "\t\t\t\t\t\tif init_latent is not None:\n",
    "\t\t\t\t\t\t\tx = init_latent + x\n",
    "\t\t\t\t\t\textra_args = {\n",
    "\t\t\t\t\t\t\t'cond': c,\n",
    "\t\t\t\t\t\t\t'uncond': uc,\n",
    "\t\t\t\t\t\t\t'cond_weights': cond_weights,\n",
    "\t\t\t\t\t\t\t'cond_arities': cond_arities,\n",
    "\t\t\t\t\t\t\t'cond_scale': opt.scale,\n",
    "\t\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t\t\tdef log_intermediate(payload: KSamplerCallbackPayload) -> None:\n",
    "\t\t\t\t\t\t\tsample_pils: List[Image.Image] = latents_to_pils(\n",
    "\t\t\t\t\t\t\t\tpayload['denoised'])\n",
    "\t\t\t\t\t\t\tfor img in sample_pils:\n",
    "\t\t\t\t\t\t\t\timg.save(os.path.join(\n",
    "\t\t\t\t\t\t\t\t\tintermediates_path, f\"inter.{payload['i']}.png\"))\n",
    "\n",
    "\t\t\t\t\t\tsamples: Tensor = sampling_fn(\n",
    "\t\t\t\t\t\t\tmodel_k_guidance,\n",
    "\t\t\t\t\t\t\tx,\n",
    "\t\t\t\t\t\t\textra_args=extra_args,\n",
    "\t\t\t\t\t\t\tcallback=log_intermediate if opt.log_intermediates else None,\n",
    "\t\t\t\t\t\t\tdisable=opt.no_progress_bars,\n",
    "\t\t\t\t\t\t\t**noise_schedule_sampler_args)\n",
    "\n",
    "\t\t\t\t\tx_samples = model.decode_first_stage(samples)\n",
    "\n",
    "\t\t\t\t\tx_samples = torch.clamp(\n",
    "\t\t\t\t\t\t(x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\t\t\t\t\tx_samples = x_samples.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "\t\t\t\t\tx_checked_image, has_nsfw_concept = check_safety_poorly(\n",
    "\t\t\t\t\t\tx_samples)\n",
    "\n",
    "\t\t\t\t\tx_checked_image_torch = torch.from_numpy(\n",
    "\t\t\t\t\t\tx_checked_image).permute(0, 3, 1, 2)\n",
    "\n",
    "\t\t\t\t\tif not opt.skip_save:\n",
    "\t\t\t\t\t\tfor x_sample, sample_seed in zip(x_checked_image_torch, sample_seeds):\n",
    "\t\t\t\t\t\t\tx_sample = 255. * \\\n",
    "\t\t\t\t\t\t\t\trearrange(x_sample.cpu().numpy(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t'c h w -> h w c')\n",
    "\t\t\t\t\t\t\timg = Image.fromarray(\n",
    "\t\t\t\t\t\t\t\tx_sample.astype(np.uint8))\n",
    "\t\t\t\t\t\t\t# img = put_watermark(img, wm_encoder)\n",
    "\t\t\t\t\t\t\tpreferred_sigmas = sigmas_quantized if sigmas_quantized is not None else sigmas\n",
    "\t\t\t\t\t\t\timg.save(os.path.join(sample_path, compute_sample_file_name(sample_seed=sample_seed, sigmas=format_sigmas_pretty(\n",
    "\t\t\t\t\t\t\t\tpreferred_sigmas, summary=True) if preferred_sigmas is not None else None)))\n",
    "\t\t\t\t\t\t\tbase_count += 1\n",
    "\n",
    "\t\t\t\t\tif not opt.skip_grid:\n",
    "\t\t\t\t\t\tall_samples.append(x_checked_image_torch)\n",
    "\t\t\t\titer_toc = time.perf_counter()\n",
    "\t\t\t\tprint(\n",
    "\t\t\t\t\tf'batch {n} generated {batch_size} images in {iter_toc-iter_tic} seconds')\n",
    "\t\t\tif not opt.skip_grid:\n",
    "\t\t\t\t# additionally, save as grid\n",
    "\t\t\t\tgrid = torch.stack(all_samples, 0)\n",
    "\t\t\t\tgrid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "\t\t\t\tgrid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "\t\t\t\t# to image\n",
    "\t\t\t\tgrid = 255. * \\\n",
    "\t\t\t\t\trearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "\t\t\t\timg = Image.fromarray(grid.astype(np.uint8))\n",
    "\t\t\t\t# img = put_watermark(img, wm_encoder)\n",
    "\t\t\t\timg.save(os.path.join(outpath, compute_batch_file_name(\n",
    "\t\t\t\t\tsigmas=format_sigmas_pretty(preferred_sigmas, summary=True))))\n",
    "\t\t\t\tgrid_count += 1\n",
    "\n",
    "\t\t\ttoc = time.perf_counter()\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf'in total, generated {opt.n_iter} batches of {batch_size} images in {toc-tic} seconds')\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "\t\tf\" \\nEnjoy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ldmwaifu-stable')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a22d57408f1be723c62f9ee9848e21c84bcf5718c8ab975b0ee92513f0b7a80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
