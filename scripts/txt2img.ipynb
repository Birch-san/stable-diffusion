{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../models/ldm/stable-diffusion-v1/model.ckpt\n",
      "Global Step: 470000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import fnmatch\n",
    "from functools import partial\n",
    "import cv2\n",
    "from cv2 import randn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor, FloatTensor, uint8\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from PIL.Image import Resampling\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice, repeat as repeat_, chain, pairwise\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast, nn\n",
    "from torch.nn import functional as F\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from random import randint\n",
    "from typing import Generic, Optional, Iterable, List, TypeAlias, Tuple, TypeVar, Callable, Protocol, TypedDict\n",
    "from types import SimpleNamespace\n",
    "import re\n",
    "from ldm.models.diffusion.ddpm import LatentDiffusion\n",
    "import abc\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from transformers import logging\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "from k_diffusion.sampling import sample_lms, sample_dpm_2, sample_dpm_2_ancestral, sample_euler, sample_euler_ancestral, sample_heun, sample_dpm_fast, sample_dpm_adaptive, get_sigmas_karras, append_zero\n",
    "from k_diffusion.external import CompVisDenoiser\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def get_device():\n",
    "\tif(torch.cuda.is_available()):\n",
    "\t\treturn 'cuda'\n",
    "\telif(torch.backends.mps.is_available()):\n",
    "\t\treturn 'mps'\n",
    "\telse:\n",
    "\t\treturn 'cpu'\n",
    "\n",
    "class KSamplerCallbackPayload(TypedDict):\n",
    "\tx: FloatTensor\n",
    "\ti: int\n",
    "\tsigma: FloatTensor\n",
    "\tsigma_hat: FloatTensor\n",
    "\tdenoised: FloatTensor\n",
    "\n",
    "KSamplerCallback: TypeAlias = Callable[[KSamplerCallbackPayload], None]\n",
    "\n",
    "T = TypeVar('T')\n",
    "Decorator: TypeAlias = Callable[[T], T]\n",
    "TensorDecorator: TypeAlias = Decorator[Tensor]\n",
    "\n",
    "class DiffusionModel(Protocol):\n",
    "\tscale_factor: float\n",
    "\tdef __call__(self, x: Tensor, sigma: Tensor, **kwargs) -> Tensor: ...\n",
    "\tdef decode_first_stage(self, latents: Tensor) -> Tensor: ...\n",
    "\tdef encode_first_stage(self, pixels: Tensor) -> Tensor: ...\n",
    "\tdef get_first_stage_encoding(self, encoded: Tensor) -> Tensor: ...\n",
    "\n",
    "class DiffusionModelMixin(DiffusionModel):\n",
    "\tscale_factor: float\n",
    "\tinner_model: DiffusionModel\n",
    "\n",
    "\t@property\n",
    "\tdef scale_factor(self) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tnotionally this is the standard deviation of latent values encoded by the autoencoder,\n",
    "\t\tacross the whole dataset upon which it learned.\n",
    "\t\tstable-diffusion just went with the standard deviation of the first batch:\n",
    "\t\t0.18215\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.inner_model.scale_factor\n",
    "\n",
    "\tdef decode_first_stage(self, latents: Tensor) -> Tensor:\n",
    "\t\treturn self.inner_model.decode_first_stage(latents)\n",
    "\n",
    "\tdef encode_first_stage(self, pixels: Tensor) -> Tensor:\n",
    "\t\treturn self.inner_model.encode_first_stage(pixels)\n",
    "\n",
    "\tdef get_first_stage_encoding(self, encoded: Tensor) -> Tensor:\n",
    "\t\treturn self.inner_model.get_first_stage_encoding(encoded)\n",
    "\n",
    "class BaseModelWrapper(nn.Module, DiffusionModelMixin):\n",
    "\tinner_model: DiffusionModel\n",
    "\n",
    "\tdef __init__(self, inner_model: DiffusionModel):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.inner_model = inner_model\n",
    "\t\tDiffusionModelMixin.__init__(self)\n",
    "\n",
    "# workaround until k-diffusion introduces official base model wrapper,\n",
    "# to make the wrapper forward all method calls to the wrapped model\n",
    "# https://github.com/crowsonkb/k-diffusion/pull/23#issuecomment-1239937951\n",
    "\n",
    "class CompVisDenoiserWrapper(CompVisDenoiser, DiffusionModelMixin):\n",
    "\tinner_model: DiffusionModel\n",
    "\n",
    "\tdef __init__(self, model: DiffusionModel, quantize=False):\n",
    "\t\tCompVisDenoiser.__init__(self, model, quantize=quantize)\n",
    "\t\tDiffusionModelMixin.__init__(self)\n",
    "\n",
    "# samplers from the Karras et al paper\n",
    "PRE_KARRAS_K_DIFF_SAMPLERS = {'k_lms', 'dpm2_ancestral', 'euler_ancestral'}\n",
    "KARRAS_SAMPLERS = {'heun', 'euler', 'dpm2'}\n",
    "DPM_SOLVER_SAMPLERS = {'dpm_fast', 'dpm_adaptive'}\n",
    "K_DIFF_SAMPLERS = {*KARRAS_SAMPLERS, *\n",
    "\t\t\t\t   PRE_KARRAS_K_DIFF_SAMPLERS, *DPM_SOLVER_SAMPLERS}\n",
    "NOT_K_DIFF_SAMPLERS = {'ddim', 'plms'}\n",
    "VALID_SAMPLERS = {*K_DIFF_SAMPLERS, *NOT_K_DIFF_SAMPLERS}\n",
    "\n",
    "class KCFGDenoiser(BaseModelWrapper):\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: Tensor,\n",
    "\t\tsigma: Tensor,\n",
    "\t\tuncond: Tensor,\n",
    "\t\tcond: Tensor,\n",
    "\t\tcond_scale: float,\n",
    "\t\tcond_arities: Iterable[int],\n",
    "\t\tcond_weights: Optional[Iterable[float]]\n",
    "\t) -> Tensor:\n",
    "\t\tif uncond is None or cond_scale == 1.0:\n",
    "\t\t\tassert cond is None or cond.size(\n",
    "\t\t\t\tdim=0) == 1, \"multi-cond guidance only implemented when CFG is in-use; please pass in an uncond, or use no more than 1 cond\"\n",
    "\t\t\treturn self.inner_model(x, sigma, cond=cond)\n",
    "\t\tuncond_count = uncond.size(dim=0)\n",
    "\t\tcond_count = cond.size(dim=0)\n",
    "\t\tcond_in = torch.cat((uncond, cond))\n",
    "\t\tdel uncond, cond\n",
    "\t\tcond_arities_tensor = torch.tensor(cond_arities, device=cond_in.device)\n",
    "\t\tx_in = cat_self_with_repeat_interleaved(\n",
    "\t\t\tt=x, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "\t\tdel x\n",
    "\t\tsigma_in = cat_self_with_repeat_interleaved(\n",
    "\t\t\tt=sigma, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "\t\tdel sigma\n",
    "\t\tuncond_out, conds_out = self.inner_model(\n",
    "\t\t\tx_in, sigma_in, cond=cond_in).split([uncond_count, cond_count])\n",
    "\t\tdel x_in, sigma_in, cond_in\n",
    "\t\tunconds = repeat_interleave_along_dim_0(\n",
    "\t\t\tt=uncond_out, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "\t\tdel cond_arities_tensor\n",
    "\t\t# transform\n",
    "\t\t#   tensor([0.5, 0.1])\n",
    "\t\t# into:\n",
    "\t\t#   tensor([[[[0.5000]]],\n",
    "\t\t#           [[[0.1000]]]])\n",
    "\t\tweight_tensor = (torch.tensor(cond_weights, device=uncond_out.device,\n",
    "\t\t\t\t\t\t dtype=unconds.dtype) * cond_scale).reshape(len(cond_weights), 1, 1, 1)\n",
    "\t\tdeltas: Tensor = (conds_out-unconds) * weight_tensor\n",
    "\t\tdel conds_out, unconds, weight_tensor\n",
    "\t\tcond = sum_along_slices_of_dim_0(deltas, arities=cond_arities)\n",
    "\t\tdel deltas\n",
    "\t\treturn uncond_out + cond\n",
    "\n",
    "def chunk(it, size):\n",
    "\tit = iter(it)\n",
    "\treturn iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "@dataclass\n",
    "class InBetweenParams(Generic[T]):\n",
    "\tfrom_: T\n",
    "\tto: T\n",
    "\tstep: float\n",
    "\n",
    "MakeInbetween: TypeAlias = Callable[[InBetweenParams], T]\n",
    "\n",
    "def intersperse_linspace(\n",
    "\tlst: List[T],\n",
    "\tmake_inbetween: MakeInbetween[T],\n",
    "\tsteps: Optional[int]\n",
    ") -> List[T]:\n",
    "\tif steps is None:\n",
    "\t\treturn lst\n",
    "\treturn [\n",
    "\t\t*chain(\n",
    "\t\t\t*(\n",
    "\t\t\t\t(\n",
    "\t\t\t\t\tpair[0],\n",
    "\t\t\t\t\t*(\n",
    "\t\t\t\t\t\tmake_inbetween(\n",
    "\t\t\t\t\t\t\tInBetweenParams(\n",
    "\t\t\t\t\t\t\t\tfrom_=pair[0],\n",
    "\t\t\t\t\t\t\t\tto=pair[1],\n",
    "\t\t\t\t\t\t\t\tstep=step\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t) for step in np.linspace(\n",
    "\t\t\t\t\t\t\tstart=1/steps,\n",
    "\t\t\t\t\t\t\tstop=1,\n",
    "\t\t\t\t\t\t\tnum=steps-1,\n",
    "\t\t\t\t\t\t\tendpoint=False\n",
    "\t\t\t\t\t\t)\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t) for pair in pairwise(lst)\n",
    "\t\t\t)\n",
    "\t\t),\n",
    "\t\tlst[-1]\n",
    "\t]\n",
    "\n",
    "def numpy_to_pil(images):\n",
    "\t\"\"\"\n",
    "\tConvert a numpy image or a batch of images to a PIL image.\n",
    "\t\"\"\"\n",
    "\tif images.ndim == 3:\n",
    "\t\timages = images[None, ...]\n",
    "\timages = (images * 255).round().astype(\"uint8\")\n",
    "\tpil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "\treturn pil_images\n",
    "\n",
    "class LatentsToPils(Protocol):\n",
    "\tdef __call__(x: Tensor) -> Tensor: List[Image.Image]\n",
    "\n",
    "def make_latents_to_pils(model: LatentDiffusion) -> LatentsToPils:\n",
    "\tdef latents_to_pils(latents: Tensor) -> List[Image.Image]:\n",
    "\t\tdecoded: Tensor = model.decode_first_stage(latents)\n",
    "\t\tclamped: Tensor = torch.clamp((decoded + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\t\tdel decoded\n",
    "\t\trearranged: Tensor = rearrange(clamped, 'b c h w -> b h w c')\n",
    "\t\tdel clamped\n",
    "\t\tdenormalized: Tensor = 255. * rearranged\n",
    "\t\tdel rearranged\n",
    "\t\trgb_images = denormalized.cpu().to(dtype=uint8).numpy()\n",
    "\t\tdel denormalized\n",
    "\t\tpils: List[Image.Image] = [Image.fromarray(\n",
    "\t\t\trgb_image) for rgb_image in rgb_images]\n",
    "\t\tdel rgb_images\n",
    "\t\treturn pils\n",
    "\treturn latents_to_pils\n",
    "\n",
    "def repeat_along_dim_0(t: Tensor, factor: int) -> Tensor:\n",
    "\t\"\"\"\n",
    "\tRepeats a tensor's contents along its 0th dim `factor` times.\n",
    "\n",
    "\trepeat_along_dim_0(torch.tensor([[0,1]]), 2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[0, 1]])\n",
    "\t# shape changes from (1, 2)\n",
    "\t#                 to (2, 2)\n",
    "\n",
    "\trepeat_along_dim_0(torch.tensor([[0,1],[2,3]]), 2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3]])\n",
    "\t# shape changes from (2, 2)\n",
    "\t#                 to (4, 2)\n",
    "\t\"\"\"\n",
    "\tassert factor >= 1\n",
    "\tif factor == 1:\n",
    "\t\treturn t\n",
    "\tif t.size(dim=0) == 1:\n",
    "\t\t# prefer expand() whenever we can, since doesn't copy\n",
    "\t\treturn t.expand(factor * t.size(dim=0), *(-1,)*(t.ndim-1))\n",
    "\treturn t.repeat((factor, *(1,)*(t.ndim-1)))\n",
    "\n",
    "def repeat_interleave_along_dim_0(t: Tensor, factors: Iterable[int], factors_tensor: Tensor, output_size: int) -> Tensor:\n",
    "\t\"\"\"\n",
    "\trepeat_interleave()s a tensor's contents along its 0th dim.\n",
    "\n",
    "\tfactors=(2,3)\n",
    "\tfactors_tensor = torch.tensor(factors)\n",
    "\toutput_size=factors_tensor.sum().item() # 5\n",
    "\tt=torch.tensor([[0,1],[2,3]])\n",
    "\trepeat_interleave_along_dim_0(t=t, factors=factors, factors_tensor=factors_tensor, output_size=output_size)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3]])\n",
    "\t\"\"\"\n",
    "\tfactors_len = len(factors)\n",
    "\tassert factors_len >= 1\n",
    "\tif len(factors) == 1:\n",
    "\t\t# prefer repeat() whenever we can, because MPS doesn't support repeat_interleave()\n",
    "\t\treturn repeat_along_dim_0(t, factors[0])\n",
    "\tif t.device.type != 'mps':\n",
    "\t\treturn t.repeat_interleave(factors_tensor, dim=0, output_size=output_size)\n",
    "\treturn torch.cat([repeat_along_dim_0(split, factor) for split, factor in zip(t.split(1, dim=0), factors)])\n",
    "\n",
    "def cat_self_with_repeat_interleaved(t: Tensor, factors: Iterable[int], factors_tensor: Tensor, output_size: int) -> Tensor:\n",
    "\t\"\"\"\n",
    "\tFast-paths for a pattern which in its worst-case looks like:\n",
    "\tt=torch.tensor([[0,1],[2,3]])\n",
    "\tfactors=(2,3)\n",
    "\ttorch.cat((t, t.repeat_interleave(factors, dim=0)))\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[2, 3]])\n",
    "\n",
    "\tFast-path:\n",
    "\t  `len(factors) == 1`\n",
    "\t  it's just a normal repeat\n",
    "\tt=torch.tensor([[0,1]])\n",
    "\tfactors=(2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[0, 1]])\n",
    "\n",
    "\tt=torch.tensor([[0,1],[2,3]])\n",
    "\tfactors=(2)\n",
    "\ttensor([[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[2, 3]])\n",
    "\t\"\"\"\n",
    "\tif len(factors) == 1:\n",
    "\t\treturn repeat_along_dim_0(t, factors[0]+1)\n",
    "\treturn torch.cat((t, repeat_interleave_along_dim_0(t=t, factors_tensor=factors_tensor, factors=factors, output_size=output_size)))\n",
    "\n",
    "def sum_along_slices_of_dim_0(t: Tensor, arities: Iterable[int]) -> Tensor:\n",
    "\t\"\"\"\n",
    "\tImplements fast-path for a pattern which in the worst-case looks like this:\n",
    "\tt=torch.tensor([[1],[2],[3]])\n",
    "\tarities=(2,1)\n",
    "\ttorch.cat([torch.sum(split, dim=0, keepdim=True) for split in t.split(arities)])\n",
    "\ttensor([[3],\n",
    "\t\t\t[3]])\n",
    "\n",
    "\tFast-path:\n",
    "\t  `len(arities) == 1`\n",
    "\t  it's just a normal sum(t, dim=0, keepdim=True)\n",
    "\tt=torch.tensor([[1],[2]])\n",
    "\tarities=(2)\n",
    "\tt.sum(dim=0, keepdim=True)\n",
    "\ttensor([[3]])\n",
    "\t\"\"\"\n",
    "\tif len(arities) == 1:\n",
    "\t\tif t.size(dim=0) == 1:\n",
    "\t\t\treturn t\n",
    "\t\treturn t.sum(dim=0, keepdim=True)\n",
    "\tsplits: List[Tensor] = t.split(arities)\n",
    "\tdel t\n",
    "\tsums: List[Tensor] = [\n",
    "\t\ttorch.sum(split, dim=0, keepdim=True) for split in splits]\n",
    "\tdel splits\n",
    "\treturn torch.cat(sums)\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "\tprint(f\"Loading model from {ckpt}\")\n",
    "\tpl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "\tif \"global_step\" in pl_sd:\n",
    "\t\tprint(f\"Global Step: {pl_sd['global_step']}\")\n",
    "\tsd = pl_sd[\"state_dict\"]\n",
    "\tmodel = instantiate_from_config(config.model)\n",
    "\tm, u = model.load_state_dict(sd, strict=False)\n",
    "\tif len(m) > 0 and verbose:\n",
    "\t\tprint(\"missing keys:\")\n",
    "\t\tprint(m)\n",
    "\tif len(u) > 0 and verbose:\n",
    "\t\tprint(\"unexpected keys:\")\n",
    "\t\tprint(u)\n",
    "\n",
    "\tmodel.to(get_device())\n",
    "\tmodel.eval()\n",
    "\treturn model\n",
    "\n",
    "def check_safety_poorly(images, **kwargs):\n",
    "\treturn images, False\n",
    "\n",
    "# https://github.com/lucidrains/imagen-pytorch/blob/ceb23d62ecf611082c82b94f2625d78084738ced/imagen_pytorch/imagen_pytorch.py#L127\n",
    "# from lucidrains' imagen_pytorch\n",
    "# MIT-licensed\n",
    "def right_pad_dims_to(x: Tensor, t: Tensor) -> Tensor:\n",
    "\tpadding_dims = x.ndim - t.ndim\n",
    "\tif padding_dims <= 0:\n",
    "\t\treturn t\n",
    "\treturn t.view(*t.shape, *((1,) * padding_dims))\n",
    "\n",
    "def load_img(path):\n",
    "\timage = Image.open(path).convert(\"RGB\")\n",
    "\tw, h = image.size\n",
    "\tprint(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "\t# resize to integer multiple of 32\n",
    "\tw, h = map(lambda x: x - x % 32, (w, h))\n",
    "\timage = image.resize((w, h), resample=Resampling.LANCZOS)\n",
    "\timage = np.array(image).astype(np.float32) / 255.0\n",
    "\timage = image[None].transpose(0, 3, 1, 2)\n",
    "\timage = torch.from_numpy(image)\n",
    "\treturn 2.*image - 1.\n",
    "\n",
    "# the idea of \"ending the noise ramp early\" (i.e. setting a high sigma_min) is that sigmas as lower as sigma_min\n",
    "# aren't very impactful, and every sigma counts when our step count is low\n",
    "# https://github.com/crowsonkb/k-diffusion/pull/23#issuecomment-1234872495\n",
    "# this is just a more performant way to get the \"sigma before sigma_min\" from a noise schedule, aka\n",
    "# get_sigmas_karras(n=steps, sigma_max=sigma_max, sigma_min=sigma_min_nominal, rho=rho)[-3]\n",
    "def get_premature_sigma_min(\n",
    "\tsteps: int,\n",
    "\tsigma_max: float,\n",
    "\tsigma_min_nominal: float,\n",
    "\trho: float\n",
    ") -> float:\n",
    "\tmin_inv_rho = sigma_min_nominal ** (1 / rho)\n",
    "\tmax_inv_rho = sigma_max ** (1 / rho)\n",
    "\tramp = (steps-2) * 1/(steps-1)\n",
    "\tsigma_min = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n",
    "\treturn sigma_min\n",
    "\n",
    "@dataclass\n",
    "class WeightedPrompt():\n",
    "\ttext: str\n",
    "\tweight: float\n",
    "\n",
    "def parse_prompt(prompt: str) -> WeightedPrompt:\n",
    "\tmatch = re.search(r\"^-?\\d+(\\.\\d*)?:\", prompt)\n",
    "\tif match is None:\n",
    "\t\treturn WeightedPrompt(text=prompt, weight=1.0)\n",
    "\tgroup = match.group()[:-1]\n",
    "\tweight = float(group)\n",
    "\treturn WeightedPrompt(text=prompt[len(group)+1:], weight=weight)\n",
    "\n",
    "MultiPrompt: TypeAlias = Iterable[WeightedPrompt]\n",
    "\n",
    "@dataclass\n",
    "class SampleSpec(abc.ABC):\n",
    "\tmultiprompt: MultiPrompt\n",
    "\n",
    "# https://stackoverflow.com/a/73107990/5257399\n",
    "class InterpStrategy(str, Enum):\n",
    "\tCondDiff = 'cond_diff'\n",
    "\tLatentSlerp = 'slerp'\n",
    "\tLatentLerp = 'lerp'\n",
    "\n",
    "\tdef __str__(self) -> str:\n",
    "\t\treturn self.value\n",
    "\n",
    "@dataclass\n",
    "class BetweenSampleSpec(SampleSpec):\n",
    "\ttarget_multiprompt: MultiPrompt\n",
    "\tinterp_quotient: float\n",
    "\n",
    "SampleSpec.register(BetweenSampleSpec)\n",
    "\n",
    "@dataclass\n",
    "class IdenticalSamplesBatchSpec():\n",
    "\tsample: SampleSpec\n",
    "\n",
    "@dataclass\n",
    "class VariedSamplesBatchSpec():\n",
    "\tsamples: List[SampleSpec]\n",
    "\n",
    "@dataclass\n",
    "class BatchSpec(abc.ABC):\n",
    "\tpass\n",
    "\n",
    "BatchSpec.register(IdenticalSamplesBatchSpec)\n",
    "BatchSpec.register(VariedSamplesBatchSpec)\n",
    "\n",
    "load_opt = SimpleNamespace(\n",
    "\tlaion400m=None,\n",
    "\tconfig='../configs/stable-diffusion/v1-inference.yaml',\n",
    "\tckpt='../models/ldm/stable-diffusion-v1/model.ckpt',\n",
    "\tembedding_path=None,\n",
    ")\n",
    "\n",
    "if load_opt.laion400m:\n",
    "\tprint(\"Falling back to LAION 400M model...\")\n",
    "\tload_opt.config = \"../configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "\tload_opt.ckpt = \"../models/ldm/text2img-large/model.ckpt\"\n",
    "\n",
    "config = OmegaConf.load(f\"{load_opt.config}\")\n",
    "model: LatentDiffusion = load_model_from_config(config, f\"{load_opt.ckpt}\")\n",
    "if load_opt.embedding_path is not None:\n",
    "\tmodel.embedding_manager.load(load_opt.embedding_path)\n",
    "\n",
    "device = torch.device(get_device())\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1527468831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmas (before quantization):\n",
      "[14.6146, 8.5600, 4.7965, 2.5508, 1.2741, 0.5895, 0.2480, 0.0923, 0.0000]\n",
      "sigmas (after quantization):\n",
      "[14.6146, 8.5700, 4.7975, 2.5482, 1.2721, 0.5884, 0.2482, 0.0936, 0.0000]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CFGDynTheshDenoiser.forward() got an unexpected keyword argument 'cond_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 657\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m sample_pils:\n\u001b[1;32m    654\u001b[0m       img\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    655\u001b[0m         intermediates_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minter.\u001b[39m\u001b[39m{\u001b[39;00mpayload[\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 657\u001b[0m   samples: Tensor \u001b[39m=\u001b[39m sampling_fn(\n\u001b[1;32m    658\u001b[0m     model_k_guidance,\n\u001b[1;32m    659\u001b[0m     x,\n\u001b[1;32m    660\u001b[0m     extra_args\u001b[39m=\u001b[39;49mextra_args,\n\u001b[1;32m    661\u001b[0m     callback\u001b[39m=\u001b[39;49mlog_intermediate \u001b[39mif\u001b[39;49;00m opt\u001b[39m.\u001b[39;49mlog_intermediates \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    662\u001b[0m     disable\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mno_progress_bars,\n\u001b[1;32m    663\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnoise_schedule_sampler_args)\n\u001b[1;32m    665\u001b[0m x_samples \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode_first_stage(samples)\n\u001b[1;32m    667\u001b[0m x_samples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(\n\u001b[1;32m    668\u001b[0m   (x_samples \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldmwaifu-stable/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/git/stable-diffusion/src/k-diffusion/k_diffusion/sampling.py:103\u001b[0m, in \u001b[0;36msample_heun\u001b[0;34m(model, x, sigmas, extra_args, callback, disable, s_churn, s_tmin, s_tmax, s_noise)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m gamma \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    102\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m eps \u001b[39m*\u001b[39m (sigma_hat \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m sigmas[i] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m--> 103\u001b[0m denoised \u001b[39m=\u001b[39m model(x, sigma_hat \u001b[39m*\u001b[39;49m s_in, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_args)\n\u001b[1;32m    104\u001b[0m d \u001b[39m=\u001b[39m to_d(x, sigma_hat, denoised)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ldmwaifu-stable/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: CFGDynTheshDenoiser.forward() got an unexpected keyword argument 'cond_weights'"
     ]
    }
   ],
   "source": [
    "def dynamic_threshold(percentile: float, floor: float, t: Tensor) -> Tensor:\n",
    "  r\"\"\"\n",
    "  Args:\n",
    "    percentile: float between 0.0 and 1.0. for example 0.995 would subject only the top 0.5%ile to clamping.\n",
    "    t: [b, c, v] tensor in pixel or latent space (where v is the result of flattening w and h)\n",
    "  \"\"\"\n",
    "  a = t.abs()\n",
    "  q = torch.quantile(a, percentile, dim=2)\n",
    "  q.clamp_(min=floor)\n",
    "  q = q.unsqueeze(2).expand(*t.shape)\n",
    "  t = t.clamp(-q, q)\n",
    "  t = t / q\n",
    "  return t\n",
    "\n",
    "cfg_scale=20.0\n",
    "\n",
    "def graph_common() -> None:\n",
    "  plt.figure(figsize=(10,2))\n",
    "  plt.ylabel('Count')\n",
    "  plt.xlabel('Latent value ÷ 0.18215')\n",
    "\n",
    "def graph_each_latent(unscaled_flattened_sample: Tensor, sigma: float, addendum: str) -> None:\n",
    "  chs = [torch.histogram(c) for c in unscaled_flattened_sample]\n",
    "  graph_common()\n",
    "  plt.title('%sPer-channel latent values after denoising sigma %.3f at CFG scale %d' % (addendum, sigma, cfg_scale))\n",
    "  for ch, col in zip(chs, ('red','green','blue','purple',)):\n",
    "    plt.hist(ch.bin_edges[:-1].cpu(), ch.bin_edges.cpu(), weights=ch.hist.cpu(), color = col, alpha = 0.25)\n",
    "  # plt.xlim(-40, 40)\n",
    "  # plt.ylim(0, 800)\n",
    "  # plt.xlim(-20, 20)\n",
    "  # plt.ylim(0, 200)\n",
    "  # plt.yscale(\"log\", base=2)\n",
    "  plt.legend(['Ch0','Ch1','Ch2','Ch3'])\n",
    "  plt.show()\n",
    "\n",
    "def graph_overall(unscaled_flattened_sample: Tensor, sigma: float) -> None:\n",
    "  h = torch.histogram(unscaled_flattened_sample)\n",
    "  graph_common()\n",
    "  plt.title('Global latent values after denoising sigma %.3f at CFG scale %d' % (sigma, cfg_scale))\n",
    "  plt.hist(h.bin_edges[:-1].cpu(), h.bin_edges.cpu(), weights=h.hist.cpu())\n",
    "  plt.legend(['All'])\n",
    "  plt.show()\n",
    "\n",
    "class DynamicThresholdingDenoiser(BaseModelWrapper):\n",
    "  apply_threshold: TensorDecorator\n",
    "  floor: float\n",
    "  ceil: float\n",
    "\n",
    "  def __init__(self, model: DiffusionModel, percentile: float, floor: float, ceil: float):\n",
    "    super().__init__(model)\n",
    "    self.apply_threshold = partial(dynamic_threshold, percentile, floor)\n",
    "    self.floor = floor\n",
    "    self.ceil = ceil\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    x: FloatTensor,\n",
    "    sigma: FloatTensor,\n",
    "    cond: FloatTensor,\n",
    "    **kwargs,\n",
    "  ) -> FloatTensor:\n",
    "    # thanks to @marunine for explaining how to apply dynamic thresholding to scaled latents\n",
    "    latents: FloatTensor = self.inner_model(x, sigma, cond=cond, **kwargs)\n",
    "    unscaled: Tensor = latents / self.scale_factor\n",
    "    graph_each_latent(unscaled[0], sigma.item(), '(before thresh) ')\n",
    "    flattened: Tensor = unscaled.flatten(2)\n",
    "    means: Tensor = flattened.mean(dim=2).unsqueeze(2)\n",
    "    recentered: Tensor = flattened-means\n",
    "    magnitudes: Tensor = recentered.abs().max()\n",
    "    if magnitudes.lt(torch.tensor(self.ceil, device=magnitudes.device)).all().item():\n",
    "      return latents\n",
    "    normalized: Tensor = recentered/magnitudes\n",
    "    thresholded: Tensor = self.apply_threshold(normalized)\n",
    "    denormalized: Tensor = thresholded*magnitudes\n",
    "    uncentered: Tensor = denormalized+means\n",
    "    graph_each_latent(uncentered[0], sigma.item(), '(after thresh) ')\n",
    "    unflattened: Tensor = uncentered.unflatten(2, latents.shape[2:])\n",
    "    scaled = unflattened * self.scale_factor\n",
    "    return scaled\n",
    "\n",
    "class CFGDenoiser(BaseModelWrapper):\n",
    "  def forward(\n",
    "    self,\n",
    "    x: FloatTensor,\n",
    "    sigma: FloatTensor,\n",
    "    cond: FloatTensor,\n",
    "    unconditional_guidance_scale: float = 1.0,\n",
    "    uncond: Optional[FloatTensor] = None,\n",
    "  ) -> FloatTensor:\n",
    "    if uncond is None or unconditional_guidance_scale == 1.0:\n",
    "      return self.inner_model(x, sigma, cond=cond)\n",
    "    cond_in = torch.cat([uncond, cond])\n",
    "    del uncond, cond\n",
    "    x_in = repeat_along_dim_0(x, cond_in.size(dim=0))\n",
    "    del x\n",
    "    sigma_in = repeat_along_dim_0(sigma, cond_in.size(dim=0))\n",
    "    del sigma\n",
    "    uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(cond_in.size(dim=0))\n",
    "    del x_in, sigma_in, cond_in\n",
    "    return uncond + (cond - uncond) * unconditional_guidance_scale\n",
    "\n",
    "class CFGDynTheshDenoiser(BaseModelWrapper):\n",
    "  apply_threshold: TensorDecorator\n",
    "  # floor: float\n",
    "  # ceil: float\n",
    "  dynamic_thresholding_percentile: float\n",
    "  dynamic_thresholding_mimic_scale: float\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    model: DiffusionModel,\n",
    "    dynamic_thresholding_percentile: float,\n",
    "    # floor: float,\n",
    "    # ceil: float,\n",
    "    dynamic_thresholding_mimic_scale\n",
    "  ):\n",
    "    super().__init__(model)\n",
    "    self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n",
    "    self.apply_threshold = partial(dynamic_threshold, dynamic_thresholding_percentile, floor=1.0)\n",
    "    self.dynamic_thresholding_mimic_scale = dynamic_thresholding_mimic_scale\n",
    "    # self.floor = floor\n",
    "    # self.ceil = ceil\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    x: FloatTensor,\n",
    "    sigma: FloatTensor,\n",
    "    cond: FloatTensor,\n",
    "    unconditional_guidance_scale: float = 1.0,\n",
    "    uncond: Optional[FloatTensor] = None,\n",
    "    **kwargs\n",
    "  ) -> FloatTensor:\n",
    "    if uncond is None or unconditional_guidance_scale == 1.0:\n",
    "      return self.inner_model(x, sigma, cond=cond)\n",
    "    cond_in = torch.cat([uncond, cond])\n",
    "    del uncond, cond\n",
    "    x_in = repeat_along_dim_0(x, cond_in.size(dim=0))\n",
    "    del x\n",
    "    sigma_in = repeat_along_dim_0(sigma, cond_in.size(dim=0))\n",
    "    del sigma\n",
    "    uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(cond_in.size(dim=0))\n",
    "    del x_in, sigma_in, cond_in\n",
    "    diff: Tensor = cond - uncond\n",
    "    dynthresh_target: Tensor = uncond + diff * self.dynamic_thresholding_mimic_scale\n",
    "\n",
    "    dt_unscaled: Tensor = dynthresh_target / self.scale_factor\n",
    "    graph_each_latent(dt_unscaled[0], sigma.item(), f'(CFG{self.dynamic_thresholding_mimic_scale:.1f}) ')\n",
    "    dt_flattened: Tensor = dt_unscaled.flatten(2)\n",
    "    dt_means: Tensor = dt_flattened.mean(dim=2).unsqueeze(2)\n",
    "    dt_recentered: Tensor = dt_flattened-dt_means\n",
    "    # dt_magnitudes: Tensor = dt_recentered.abs().max()\n",
    "    # normalized: Tensor = dt_recentered/dt_magnitudes\n",
    "    dt_q = torch.quantile(dt_recentered.abs(), self.dynamic_thresholding_percentile, dim=2)\n",
    "\n",
    "    ut: Tensor = uncond + diff * unconditional_guidance_scale\n",
    "    ut_unscaled: Tensor = dynthresh_target / self.scale_factor\n",
    "    graph_each_latent(ut_unscaled[0], sigma.item(), f'(before thresh, CFG{unconditional_guidance_scale:.1f}) ')\n",
    "    ut_flattened: Tensor = ut_unscaled.flatten(2)\n",
    "    ut_means: Tensor = ut_flattened.mean(dim=2).unsqueeze(2)\n",
    "    ut_recentered: Tensor = ut_flattened-ut_means\n",
    "\n",
    "    a = ut.abs()\n",
    "    ut_q = torch.quantile(a, self.dynamic_thresholding_percentile, dim=2)\n",
    "    ut_q.clamp_(min=dt_q)\n",
    "    q_ratio = ut_q / dt_q\n",
    "    # ut_q = ut_q.unsqueeze(2).expand(*ut.shape)\n",
    "    t = ut.clamp(-dt_q, dt_q)\n",
    "    # t = t / ut_q\n",
    "    t = t / q_ratio\n",
    "\n",
    "    return t\n",
    "\n",
    "class MultiCondCFGDenoiser(BaseModelWrapper):\n",
    "  def forward(\n",
    "    self,\n",
    "    x: Tensor,\n",
    "    sigma: Tensor,\n",
    "    uncond: Tensor,\n",
    "    cond: Tensor,\n",
    "    cond_scale: float,\n",
    "    cond_arities: Iterable[int],\n",
    "    cond_weights: Optional[Iterable[float]]\n",
    "  ) -> Tensor:\n",
    "    if uncond is None or cond_scale == 1.0:\n",
    "      assert cond is None or cond.size(\n",
    "        dim=0) == 1, \"multi-cond guidance only implemented when CFG is in-use; please pass in an uncond, or use no more than 1 cond\"\n",
    "      return self.inner_model(x, sigma, cond=cond)\n",
    "    uncond_count = uncond.size(dim=0)\n",
    "    cond_count = cond.size(dim=0)\n",
    "    cond_in = torch.cat((uncond, cond))\n",
    "    del uncond, cond\n",
    "    cond_arities_tensor = torch.tensor(cond_arities, device=cond_in.device)\n",
    "    x_in = cat_self_with_repeat_interleaved(\n",
    "      t=x, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "    del x\n",
    "    sigma_in = cat_self_with_repeat_interleaved(\n",
    "      t=sigma, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "    del sigma\n",
    "    uncond_out, conds_out = self.inner_model(\n",
    "      x_in, sigma_in, cond=cond_in).split([uncond_count, cond_count])\n",
    "    del x_in, sigma_in, cond_in\n",
    "    unconds = repeat_interleave_along_dim_0(\n",
    "      t=uncond_out, factors_tensor=cond_arities_tensor, factors=cond_arities, output_size=cond_count)\n",
    "    del cond_arities_tensor\n",
    "    # transform\n",
    "    #   tensor([0.5, 0.1])\n",
    "    # into:\n",
    "    #   tensor([[[[0.5000]]],\n",
    "    #           [[[0.1000]]]])\n",
    "    weight_tensor = (torch.tensor(cond_weights, device=uncond_out.device,\n",
    "             dtype=unconds.dtype) * cond_scale).reshape(len(cond_weights), 1, 1, 1)\n",
    "    deltas: Tensor = (conds_out-unconds) * weight_tensor\n",
    "    del conds_out, unconds, weight_tensor\n",
    "    cond = sum_along_slices_of_dim_0(deltas, arities=cond_arities)\n",
    "    del deltas\n",
    "    return uncond_out + cond\n",
    "\n",
    "opt = SimpleNamespace(\n",
    "  **load_opt.__dict__,\n",
    "  prompt=[['masterpiece character portrait of shrine maiden, artgerm, ilya kuvshinov, tony pyykko, from side, looking at viewer, long black hair, upper body, 4k hdr, global illumination, lit from behind, oriental scenic, Pixiv featured, vaporwave']],\n",
    "  prompt_interpolation_steps=None,\n",
    "  prompt_interpolation_strategy=InterpStrategy.CondDiff,\n",
    "  outdir='../outputs/txt2img-samples',\n",
    "  skip_grid=True,\n",
    "  skip_save=None,\n",
    "  steps=8,\n",
    "  sampler='heun',\n",
    "  karras_noise=True,\n",
    "  end_noise_ramp_early=True,\n",
    "  sigma_max=None,\n",
    "  sigma_min=None,\n",
    "  rho=7.,\n",
    "  churn=0.,\n",
    "  dynamic_thresholding=True,\n",
    "  dynamic_thresholding_mimic_scale=7.5,\n",
    "  dynamic_thresholding_percentile=0.9995,#0.9,\n",
    "  # actually it runs on normalized latent values now (e.g. ≤±1.), so setting the floor this high may be a mistake, perhaps even harmful. still seems to be better than not thresholding lol.\n",
    "  dynamic_thresholding_floor=3.0560, #48.3, #8.8,\n",
    "  # dynamic_thresholding_ceil=48.3, #48.3, #8.8,\n",
    "  dynamic_thresholding_ceil=42., #48.3, #8.8,\n",
    "  fixed_code=None,\n",
    "  fixed_code_within_batch=None,\n",
    "  ddim_eta=0.0,\n",
    "  n_iter=1,\n",
    "  H=512,\n",
    "  W=512,\n",
    "  C=4,\n",
    "  f=8,\n",
    "  n_samples=1,\n",
    "  n_rows=0,\n",
    "  # scale=7.5,\n",
    "  scale=cfg_scale,\n",
    "  from_file=None,\n",
    "  seed=1527468831,\n",
    "  # seed=679566949,\n",
    "  precision='autocast',\n",
    "  filename_prompt=True,\n",
    "  filename_sample_ix=None,\n",
    "  filename_seed=True,\n",
    "  filename_sampling=True,\n",
    "  filename_guidance=None,\n",
    "  filename_sigmas=None,\n",
    "  log_intermediates=True,\n",
    "  no_progress_bars=True,\n",
    "  init_img=None,\n",
    "  strength=0.75,\n",
    ")\n",
    "\n",
    "if opt.laion400m:\n",
    "  opt.outdir = f\"{opt.outdir}-laion400m\"\n",
    "\n",
    "latents_to_pils: LatentsToPils = make_latents_to_pils(model)\n",
    "\n",
    "if opt.sampler in K_DIFF_SAMPLERS:\n",
    "  model_k_wrapped = CompVisDenoiserWrapper(model, quantize=True)\n",
    "  # model_k_guidance = KCFGDenoiser(model_k_wrapped)\n",
    "  if opt.dynamic_thresholding:\n",
    "    # messy, coupled approach for now.\n",
    "    model_k_guidance = CFGDynTheshDenoiser(\n",
    "      model_k_wrapped,\n",
    "      opt.dynamic_thresholding_percentile,\n",
    "      # opt.dynamic_thresholding_floor,\n",
    "      # opt.dynamic_thresholding_ceil,\n",
    "      opt.dynamic_thresholding_mimic_scale,\n",
    "    )\n",
    "  else:\n",
    "    # model_k_guidance = MultiCondCFGDenoiser(model_k_wrapped)\n",
    "    model_k_guidance = CFGDenoiser(model_k_wrapped)\n",
    "elif opt.sampler in NOT_K_DIFF_SAMPLERS:\n",
    "  if opt.sampler == 'plms':\n",
    "    sampler = PLMSSampler(model)\n",
    "  else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "# commenting-out decorator approach whilst I experiment with an approach that's weaved in a bit more tightly\n",
    "# if opt.dynamic_thresholding:\n",
    "#   model_k_guidance = DynamicThresholdingDenoiser(\n",
    "#     model_k_guidance,\n",
    "#     opt.dynamic_thresholding_percentile,\n",
    "#     opt.dynamic_thresholding_floor,\n",
    "#     opt.dynamic_thresholding_ceil,\n",
    "#   )\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples\n",
    "n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "\n",
    "make_inbetween: MakeInbetween[SampleSpec] = lambda params: BetweenSampleSpec(\n",
    "  interp_quotient=params.step,\n",
    "  multiprompt=params.from_.multiprompt,\n",
    "  target_multiprompt=params.to.multiprompt\n",
    ")\n",
    "\n",
    "prompts_change_each_batch = bool(opt.from_file) or len(opt.prompt) > 1\n",
    "batch_specs: Iterable[BatchSpec] = None\n",
    "if opt.from_file:\n",
    "  print(f\"reading prompts from {opt.from_file}\")\n",
    "  with open(opt.from_file, \"r\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    batch_specs = [\n",
    "      VariedSamplesBatchSpec(samples=batch) for batch in chunk(\n",
    "        intersperse_linspace(\n",
    "          [\n",
    "            # every line in the file is considered to be a single multiprompt.\n",
    "            SampleSpec(\n",
    "              # splitting the line on tab, gives each prompt of the multiprompt.\n",
    "              multiprompt=[parse_prompt(\n",
    "                subprompt) for subprompt in line.split('\\t')]\n",
    "            ) for line in lines\n",
    "          ],\n",
    "          make_inbetween=make_inbetween,\n",
    "          steps=opt.prompt_interpolation_steps\n",
    "        ),\n",
    "        batch_size\n",
    "      )\n",
    "    ]\n",
    "elif len(opt.prompt) == 1:\n",
    "  # fast-path for the common case where just one prompt is provided\n",
    "  batch_specs = [IdenticalSamplesBatchSpec(\n",
    "    sample=SampleSpec(\n",
    "      multiprompt=[parse_prompt(subprompt)\n",
    "              for subprompt in opt.prompt[0]]\n",
    "    )\n",
    "  )]\n",
    "else:\n",
    "  batch_specs = [\n",
    "    VariedSamplesBatchSpec(samples=batch) for batch in chunk(\n",
    "      intersperse_linspace(\n",
    "        [\n",
    "          SampleSpec(\n",
    "            multiprompt=[parse_prompt(subprompt)\n",
    "                    for subprompt in multiprompt]\n",
    "          ) for multiprompt in opt.prompt\n",
    "        ],\n",
    "        make_inbetween=make_inbetween,\n",
    "        steps=opt.prompt_interpolation_steps\n",
    "      ),\n",
    "      batch_size\n",
    "    )\n",
    "  ]\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "intermediates_path = os.path.join(outpath, \"intermediates\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "os.makedirs(intermediates_path, exist_ok=True)\n",
    "base_count = len(fnmatch.filter(\n",
    "  os.listdir(sample_path), f\"{5*'[0-9]'}.*.png\"))\n",
    "grid_count = len(fnmatch.filter(\n",
    "  os.listdir(outpath), f\"grid-{4*'[0-9]'}.*.png\"))\n",
    "\n",
    "shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "\n",
    "start_code = None\n",
    "\n",
    "karras_noise_active = False\n",
    "end_karras_ramp_early_active = False\n",
    "\n",
    "def _format_sigma_pretty(sigma: Tensor) -> str:\n",
    "  return \"%.4f\" % sigma\n",
    "\n",
    "def format_sigmas_pretty(sigmas: Tensor, summary: bool = False) -> str:\n",
    "  if sigmas is None:\n",
    "    return '[]'\n",
    "  if summary and sigmas.size(dim=0) > 9:\n",
    "    start = \", \".join(_format_sigma_pretty(sigma)\n",
    "              for sigma in sigmas[0:4])\n",
    "    end = \", \".join(_format_sigma_pretty(sigma)\n",
    "            for sigma in sigmas[-4:])\n",
    "    return f'[{start}, …, {end}]'\n",
    "  return f'[{\", \".join(_format_sigma_pretty(sigma) for sigma in sigmas)}]'\n",
    "\n",
    "def _compute_common_file_name_portion(seed: int, sigmas: str = '') -> str:\n",
    "  seed_ = ''\n",
    "  sampling = ''\n",
    "  prompt = ''\n",
    "  sigmas_ = ''\n",
    "  guidance = ''\n",
    "  if opt.filename_sampling:\n",
    "    kna = '_kns' if karras_noise_active else ''\n",
    "    nz = '_ek' if end_karras_ramp_early_active else ''\n",
    "    sampling = f\"{opt.sampler}{opt.steps}{kna}{nz}\"\n",
    "  if opt.filename_seed:\n",
    "    seed_ = f\".s{seed}\"\n",
    "  if opt.filename_prompt:\n",
    "    sanitized = re.sub(r\"[/\\\\?%*:|\\\"<>\\x7F\\x00-\\x1F]\",\n",
    "              \"-\", '_'.join(opt.prompt[0]))\n",
    "    prompt = f\"_{sanitized}_\"\n",
    "  if opt.filename_sigmas and sigmas is not None:\n",
    "    sigmas_ = f\"_{sigmas}_\"\n",
    "  if opt.filename_guidance:\n",
    "    guidance = f\"_str{opt.strength}_sca{opt.scale}\"\n",
    "  nominal = f\"{seed_}{prompt}{sigmas_}{guidance}{sampling}\"\n",
    "  # https://apple.stackexchange.com/a/86617/251820\n",
    "  # macOS imposes a filename limit of ~255 chars\n",
    "  # we already used up some on base_count and the file extension\n",
    "  # shed the biggest parts if we must, so that saving doesn't go bang\n",
    "  if len(nominal) > 245:\n",
    "    nominal = f\"{seed_}{prompt}{guidance}{sampling}\"\n",
    "  if len(nominal) > 245:\n",
    "    nominal = f\"{seed_}{guidance}{sampling}\"\n",
    "  return nominal\n",
    "\n",
    "def compute_batch_file_name(sigmas: str = '') -> str:\n",
    "  common_file_name_portion = _compute_common_file_name_portion(\n",
    "    seed=opt.seed, sigmas=sigmas)\n",
    "  return f\"grid-{grid_count:04}{common_file_name_portion}.png\"\n",
    "\n",
    "def compute_sample_file_name(sample_seed: int, sigmas: Optional[str] = None) -> str:\n",
    "  common_file_name_portion = _compute_common_file_name_portion(\n",
    "    seed=sample_seed, sigmas=sigmas)\n",
    "  return f\"{base_count:05}{common_file_name_portion}.png\"\n",
    "\n",
    "def img_to_latent(path: str) -> Tensor:\n",
    "  assert os.path.isfile(path)\n",
    "  image = load_img(path).to(device)\n",
    "  image = repeat(image, '1 ... -> b ...', b=batch_size)\n",
    "  latent: Tensor = model.get_first_stage_encoding(\n",
    "    model.encode_first_stage(image))  # move to latent space\n",
    "  return latent\n",
    "\n",
    "init_latent = None\n",
    "if opt.init_img:\n",
    "  init_latent = img_to_latent(opt.init_img)\n",
    "t_enc = int((1.0-opt.strength) * opt.steps)\n",
    "\n",
    "precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
    "if device.type == 'mps':\n",
    "  precision_scope = nullcontext  # have to use f32 on mps\n",
    "with torch.no_grad():\n",
    "  with precision_scope(device.type):\n",
    "    with model.ema_scope():\n",
    "      tic = time.perf_counter()\n",
    "      all_samples = list()\n",
    "      uc: Optional[FloatTensor] = None if opt.scale == 1.0 else model.get_learned_conditioning(\n",
    "        \"\").expand(batch_size, -1, -1)\n",
    "      c: Optional[FloatTensor] = None\n",
    "      cond_weights: Optional[Iterable[float]] = None\n",
    "      cond_arities: Optional[Iterable[int]] = None\n",
    "      sample_seeds: Optional[Iterable[int]] = None\n",
    "      for n in range(opt.n_iter): #, desc=\"Iterations\", disable=opt.no_progress_bars):\n",
    "        iter_tic = time.perf_counter()\n",
    "        for batch_spec in batch_specs: #tqdm(batch_specs, desc=f\"Iteration {n}, batch\", disable=opt.no_progress_bars):\n",
    "          if start_code is None or not opt.fixed_code:\n",
    "            first_sample_of_batch_seed = opt.seed if opt.seed is not None and (\n",
    "              opt.fixed_code or start_code is None) else randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "            if opt.fixed_code_within_batch:\n",
    "              sample_seeds: Iterable[int] = (\n",
    "                first_sample_of_batch_seed,) * opt.n_samples\n",
    "              seed_everything(first_sample_of_batch_seed)\n",
    "              # https://github.com/CompVis/stable-diffusion/issues/25#issuecomment-1229706811\n",
    "              # MPS random is not currently deterministic w.r.t seed, so compute randn() on-CPU\n",
    "              sample_start_code = torch.randn(shape, device='cpu').to(\n",
    "                device) if device.type == 'mps' else torch.randn(shape, device=device)\n",
    "              start_code = sample_start_code.unsqueeze(\n",
    "                0).expand(opt.n_samples)\n",
    "            else:\n",
    "              if opt.n_samples == 1:\n",
    "                sample_seeds: Iterable[int] = (\n",
    "                  first_sample_of_batch_seed,)\n",
    "              else:\n",
    "                sample_seeds: Iterable[int] = (first_sample_of_batch_seed,) + tuple(torch.randint(\n",
    "                  low=np.iinfo(np.uint32).min,\n",
    "                  high=np.iinfo(np.uint32).max,\n",
    "                  dtype=torch.int64,\n",
    "                  device='cpu',\n",
    "                  size=(opt.n_samples-1,)\n",
    "                ).numpy())\n",
    "              sample_start_codes = []\n",
    "              for seed in sample_seeds:\n",
    "                seed_everything(seed)\n",
    "                # https://github.com/CompVis/stable-diffusion/issues/25#issuecomment-1229706811\n",
    "                # MPS random is not currently deterministic w.r.t seed, so compute randn() on-CPU\n",
    "                sample_start_codes.append(torch.randn(shape, device='cpu').to(\n",
    "                  device) if device.type == 'mps' else torch.randn(shape, device=device))\n",
    "              start_code = torch.stack(\n",
    "                sample_start_codes, dim=0)\n",
    "\n",
    "          if c is None or prompts_change_each_batch:\n",
    "            match batch_spec:\n",
    "              case IdenticalSamplesBatchSpec(sample):\n",
    "                # for some reason Python isn't narrowing the type automatically\n",
    "                sample: SampleSpec = sample\n",
    "                texts: List[str] = [\n",
    "                  subprompt.text for subprompt in sample.multiprompt]\n",
    "                cond_arities: Tuple[int, ...] = (\n",
    "                  len(texts),) * batch_size\n",
    "                cond_weights: List[float] = [\n",
    "                  subprompt.weight for subprompt in sample.multiprompt] * batch_size\n",
    "                p = model.get_learned_conditioning(texts)\n",
    "                c = repeat_along_dim_0(p, batch_size)\n",
    "                del p\n",
    "              case VariedSamplesBatchSpec(samples):\n",
    "                # for some reason Python isn't narrowing the type automatically\n",
    "                samples: List[SampleSpec] = samples\n",
    "                assert len(samples) == batch_size\n",
    "                texts: List[str] = [subprompt.text for sample in samples for subprompt in (\n",
    "                  (*sample.multiprompt, *sample.target_multiprompt) if isinstance(sample, BetweenSampleSpec) else sample.multiprompt)]\n",
    "                cond_weights: List[float] = [interp_quotient*subprompt.weight for sample in samples for interp_quotient, multiprompt in (((1-sample.interp_quotient, sample.multiprompt), (\n",
    "                  sample.interp_quotient, sample.target_multiprompt)) if isinstance(sample, BetweenSampleSpec) else ((1., sample.multiprompt),)) for subprompt in multiprompt]\n",
    "                cond_arities: List[int] = [len(sample.multiprompt) + len(sample.target_multiprompt) if isinstance(\n",
    "                  sample, BetweenSampleSpec) else len(sample.multiprompt) for sample in samples]\n",
    "                c = model.get_learned_conditioning(texts)\n",
    "              case _:\n",
    "                raise TypeError(\n",
    "                  f\"That ({batch_spec}) ain't no BatchSpec I ever heard of\")\n",
    "\n",
    "          if opt.sampler in NOT_K_DIFF_SAMPLERS:\n",
    "            if opt.karras_noise:\n",
    "              print(\n",
    "                f\"[WARN] You have requested --karras_noise, but Karras et al noise schedule is not implemented for {opt.sampler} sampler. Implemented only for {K_DIFF_SAMPLERS}. Using default noise schedule from DDIM.\")\n",
    "            if init_latent is None:\n",
    "              samples, _ = sampler.sample(\n",
    "                S=opt.steps,\n",
    "                conditioning=c,\n",
    "                batch_size=opt.n_samples,\n",
    "                shape=shape,\n",
    "                verbose=False,\n",
    "                unconditional_guidance_scale=opt.scale,\n",
    "                unconditional_conditioning=uc,\n",
    "                eta=opt.ddim_eta,\n",
    "                x_T=start_code\n",
    "              )\n",
    "              # for PLMS and DDIM, sigmas are all 0\n",
    "              sigmas = None\n",
    "              sigmas_quantized = None\n",
    "            else:\n",
    "              z_enc = sampler.stochastic_encode(\n",
    "                init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "              samples = sampler.decode(\n",
    "                z_enc,\n",
    "                c,\n",
    "                t_enc,\n",
    "                unconditional_guidance_scale=opt.scale,\n",
    "                unconditional_conditioning=uc,\n",
    "              )\n",
    "          elif opt.sampler in K_DIFF_SAMPLERS:\n",
    "            match opt.sampler:\n",
    "              case 'dpm_fast':\n",
    "                sampling_fn = sample_dpm_fast\n",
    "              case 'dpm_adaptive':\n",
    "                sampling_fn = sample_dpm_adaptive\n",
    "              case 'dpm2':\n",
    "                sampling_fn = sample_dpm_2\n",
    "              case 'dpm2_ancestral':\n",
    "                sampling_fn = sample_dpm_2_ancestral\n",
    "              case 'heun':\n",
    "                sampling_fn = sample_heun\n",
    "              case 'euler':\n",
    "                sampling_fn = sample_euler\n",
    "              case 'euler_ancestral':\n",
    "                sampling_fn = sample_euler_ancestral\n",
    "              case 'k_lms' | _:\n",
    "                sampling_fn = sample_lms\n",
    "\n",
    "            sigmas = None\n",
    "            sigmas_quantized = None\n",
    "            noise_schedule_sampler_args = {}\n",
    "            if opt.karras_noise or opt.sampler in DPM_SOLVER_SAMPLERS:\n",
    "              # 0.0292\n",
    "              sigma_min = opt.sigma_min or model_k_wrapped.sigma_min.item()\n",
    "              # 14.6146\n",
    "              sigma_max = opt.sigma_max or model_k_wrapped.sigma_max.item()\n",
    "              if opt.end_noise_ramp_early and not opt.sigma_min:\n",
    "                # get the \"sigma before sigma_min\" from a slightly longer ramp\n",
    "                # https://github.com/crowsonkb/k-diffusion/pull/23#issuecomment-1234872495\n",
    "                sigma_min = get_premature_sigma_min(\n",
    "                  steps=opt.steps+1,\n",
    "                  sigma_max=sigma_max,\n",
    "                  sigma_min_nominal=model_k_wrapped.sigma_min.item(),\n",
    "                  rho=opt.rho\n",
    "                )\n",
    "\n",
    "              # Karras sampling schedule achieves higher FID in fewer steps\n",
    "              # https://arxiv.org/abs/2206.00364\n",
    "              if opt.sampler not in DPM_SOLVER_SAMPLERS:\n",
    "                if opt.sampler not in KARRAS_SAMPLERS:\n",
    "                  print(\n",
    "                    f\"[WARN] you have enabled --karras_noise, but you are using it with a sampler ({opt.sampler}) outside of the ones proposed in the same paper (arXiv:2206.00364), {KARRAS_SAMPLERS}. No idea what results you'll get.\")\n",
    "\n",
    "                sigmas = get_sigmas_karras(\n",
    "                  n=opt.steps,\n",
    "                  sigma_min=sigma_min,\n",
    "                  sigma_max=sigma_max,\n",
    "                  rho=opt.rho,\n",
    "                  device=device,\n",
    "                )\n",
    "                karras_noise_active = True\n",
    "                end_karras_ramp_early_active = opt.end_noise_ramp_early\n",
    "            else:\n",
    "              if opt.sampler in KARRAS_SAMPLERS:\n",
    "                print(\n",
    "                  f\"[WARN] you should really enable --karras_noise for best results; it's the noise schedule proposed in the same paper (arXiv:2206.00364) as the sampler you're using ({opt.sampler}). Falling back to default k-diffusion get_sigmas() noise schedule.\")\n",
    "              sigmas = model_k_wrapped.get_sigmas(opt.steps)\n",
    "\n",
    "            if opt.sampler in KARRAS_SAMPLERS:\n",
    "              noise_schedule_sampler_args['s_churn'] = opt.churn\n",
    "\n",
    "            if init_latent is not None:\n",
    "              assert opt.sampler not in DPM_SOLVER_SAMPLERS, \"img2img not yet implemented for DPM-Solver samplers -- need to figure out how to skip a portion of the noise schedule\"\n",
    "              sigmas = sigmas[len(sigmas) - t_enc - 1:]\n",
    "\n",
    "            if opt.sampler in DPM_SOLVER_SAMPLERS:\n",
    "              noise_schedule_sampler_args['sigma_min'] = sigma_min\n",
    "              noise_schedule_sampler_args['sigma_max'] = sigma_max\n",
    "              if opt.sampler == 'dpm_fast':\n",
    "                noise_schedule_sampler_args['n'] = opt.steps\n",
    "            else:\n",
    "              noise_schedule_sampler_args['sigmas'] = sigmas\n",
    "              print('sigmas (before quantization):')\n",
    "              print(format_sigmas_pretty(sigmas))\n",
    "              print('sigmas (after quantization):')\n",
    "              sigmas_quantized = append_zero(model_k_wrapped.sigmas[torch.argmin(\n",
    "                (sigmas[:-1].reshape(len(sigmas)-1, 1).repeat(1, len(model_k_wrapped.sigmas)) - model_k_wrapped.sigmas).abs(), dim=1)])\n",
    "              print(format_sigmas_pretty(sigmas_quantized))\n",
    "\n",
    "            first_sigma = sigma_max if opt.sampler in DPM_SOLVER_SAMPLERS else sigmas[\n",
    "              0]\n",
    "            x = start_code * first_sigma\n",
    "            if init_latent is not None:\n",
    "              x = init_latent + x\n",
    "            extra_args = {\n",
    "              'cond': c,\n",
    "              'uncond': uc,\n",
    "              'cond_weights': cond_weights,\n",
    "              'cond_arities': cond_arities,\n",
    "              'cond_scale': opt.scale,\n",
    "            }\n",
    "\n",
    "            def log_intermediate(payload: KSamplerCallbackPayload) -> None:\n",
    "              sample_pils: List[Image.Image] = latents_to_pils(\n",
    "                payload['denoised'])\n",
    "              for img in sample_pils:\n",
    "                img.save(os.path.join(\n",
    "                  intermediates_path, f\"inter.{payload['i']}.png\"))\n",
    "\n",
    "            samples: Tensor = sampling_fn(\n",
    "              model_k_guidance,\n",
    "              x,\n",
    "              extra_args=extra_args,\n",
    "              callback=log_intermediate if opt.log_intermediates else None,\n",
    "              disable=opt.no_progress_bars,\n",
    "              **noise_schedule_sampler_args)\n",
    "\n",
    "          x_samples = model.decode_first_stage(samples)\n",
    "\n",
    "          x_samples = torch.clamp(\n",
    "            (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "          x_samples = x_samples.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "          x_checked_image, has_nsfw_concept = check_safety_poorly(\n",
    "            x_samples)\n",
    "\n",
    "          x_checked_image_torch = torch.from_numpy(\n",
    "            x_checked_image).permute(0, 3, 1, 2)\n",
    "\n",
    "          if not opt.skip_save:\n",
    "            for x_sample, sample_seed in zip(x_checked_image_torch, sample_seeds):\n",
    "              x_sample = 255. * \\\n",
    "                rearrange(x_sample.cpu().numpy(),\n",
    "                      'c h w -> h w c')\n",
    "              img = Image.fromarray(\n",
    "                x_sample.astype(np.uint8))\n",
    "              # img = put_watermark(img, wm_encoder)\n",
    "              preferred_sigmas = sigmas_quantized if sigmas_quantized is not None else sigmas\n",
    "              img.save(os.path.join(sample_path, compute_sample_file_name(sample_seed=sample_seed, sigmas=format_sigmas_pretty(\n",
    "                preferred_sigmas, summary=True) if preferred_sigmas is not None else None)))\n",
    "              base_count += 1\n",
    "\n",
    "          if not opt.skip_grid:\n",
    "            all_samples.append(x_checked_image_torch)\n",
    "        iter_toc = time.perf_counter()\n",
    "        print(\n",
    "          f'batch {n} generated {batch_size} images in {iter_toc-iter_tic} seconds')\n",
    "      if not opt.skip_grid:\n",
    "        # additionally, save as grid\n",
    "        grid = torch.stack(all_samples, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "        # to image\n",
    "        grid = 255. * \\\n",
    "          rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        img = Image.fromarray(grid.astype(np.uint8))\n",
    "        # img = put_watermark(img, wm_encoder)\n",
    "        img.save(os.path.join(outpath, compute_batch_file_name(\n",
    "          sigmas=format_sigmas_pretty(preferred_sigmas, summary=True))))\n",
    "        grid_count += 1\n",
    "\n",
    "      toc = time.perf_counter()\n",
    "      print(\n",
    "        f'in total, generated {opt.n_iter} batches of {batch_size} images in {toc-tic} seconds')\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "    f\" \\nEnjoy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.hist([1, 2, 3, 2, 1], bins=3,)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ldmwaifu-stable')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a22d57408f1be723c62f9ee9848e21c84bcf5718c8ab975b0ee92513f0b7a80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
